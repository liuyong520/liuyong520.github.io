---
title: 分布式系统之冗余数据一致性
date: 2018-05-04 13:34:47
tags:
  - 分布式系统
categories:
  - 分布式系统
toc: true
comment: true
---

# 一，为什么要冗余数据
互联网数据量很大的业务场景，往往数据库需要进行水平切分来降低单库数据量。

水平切分会有一个patition key，通过patition key的查询能够直接定位到库，但是非patition key上的查询可能就需要扫描多个库了。

此时常见的架构设计方案，是使用数据冗余这种反范式设计来满足分库后不同维度的查询需求。

例如：订单业务，对用户和商家都有查询需求：
Order(oid, info_detail);
T(buyer_id, seller_id, oid);
如果用buyer_id来分库，seller_id的查询就需要扫描多库。
如果用seller_id来分库，buyer_id的查询就需要扫描多库。

此时可以使用数据冗余来分别满足buyer_id和seller_id上的查询需求：
T1(buyer_id, seller_id, oid)
T2(seller_id, buyer_id, oid)
同一个数据，冗余两份，一份以buyer_id来分库，满足买家的查询需求；一份以seller_id来分库，满足卖家的查询需求。

如何实施数据的冗余，以及如何保证数据的一致性，是今天将要讨论的内容。
 
# 二，如何进行数据冗余
## 服务同步双写
![enter description here](https://www.github.com/liuyong520/pic/raw/master/小书匠/1559541736658.png)

顾名思义，由服务层同步写冗余数据，如上图1-4流程：
- 业务方调用服务，新增数据
- 服务先插入DB1数据
- 服务再插入DB2数据
- 服务返回业务方新增数据成功

**优点**：
不复杂，服务层由单次写，变两次写
数据一致性相对较高（因为双写成功才返回）

**缺点**：
请求的处理时间增加（要插入两次，时间加倍）
数据仍可能不一致，例如第二步写入T1完成后服务重启，则数据不会写入T2

如果系统对处理时间比较敏感，引出常用的第二种方案。

## 服务异步双写
![enter description here](https://www.github.com/liuyong520/pic/raw/master/小书匠/1559541777225.png)
数据的双写并不再由服务来完成，服务层异步发出一个消息，通过消息总线发送给一个专门的数据复制服务来写入冗余数据，如上图1-6流程：
- 业务方调用服务，新增数据
- 服务先插入DB1数据
- 服务向消息总线发送一个异步消息（发出即可，不用等返回，通常很快就能完成）
- 服务返回业务方新增数据成功
- 消息总线将消息投递给数据同步中心
- 数据同步中心插入DB2数据

**优点**：
- 请求处理时间短（只插入1次）

**缺点**：
- 系统的复杂性增加了，多引入了一个组件（消息总线）和一个服务（专用的数据复制服务）
- 因为返回业务线数据插入成功时，数据还不一定插入到T2中，因此数据有一个不一致时间窗口（这个窗口很短，最终是一致的）
- 在消息总线丢失消息时，冗余表数据会不一致

不管是服务同步双写，还是服务异步双写，服务都需要关注“冗余数据”带来的复杂性。如果想解除“数据冗余”对系统的耦合，引出常用的第三种方案。

## 线下异步双写
![enter description here](https://www.github.com/liuyong520/pic/raw/master/小书匠/1559542589601.png)
为了屏蔽“冗余数据”对服务带来的复杂性，数据的双写不再由服务层来完成，而是由线下的一个服务或者任务来完成，如上图1-6流程：
- 业务方调用服务，新增数据
- 服务先插入DB1数据
- 服务返回业务方新增数据成功
- 数据会被写入到数据库的log中
- 线下服务或者任务读取数据库的log
- 线下服务或者任务插入DB2数据

**优点**：
- 数据双写与业务完全解耦
- 请求处理时间短（只插入1次）

**缺点**：
- 返回业务线数据插入成功时，数据还不一定插入到T2中，因此数据有一个不一致时间窗口（这个窗口很短，最终是一致的）
- 数据的一致性依赖于线下服务或者任务的可靠性

不管哪种方案，毕竟不是分布式事务，万一出现数据不一致，怎么办呢？
高并发的情况下，实时一致性很难，方法论是：**最终一致性**。
实现方式是：**异步检测，异步修复**。

# 三，如何保证数据的一致性
## 线下扫描全量数据法
![enter description here](https://www.github.com/liuyong520/pic/raw/master/小书匠/1559543150549.png)
如上图所示，线下启动一个离线的扫描工具，不停的比对正表DB1和反表DB2，如果发现数据不一致，就进行补偿修复。

**优点**：
- 比较简单，开发代价小
- 线上服务无需修改，修复工具与线上服务解耦

**缺点**：
- 扫描效率低，会扫描大量的“已经能够保证一致”的数据
- 由于扫描的数据量大，扫描一轮的时间比较长，即数据如果不一致，不一致的时间窗口比较长
 
有没有只扫描“可能存在不一致可能性”的数据，而不是每次扫描全部数据，以提高效率的优化方法呢？

## 线下扫描增量数据法
![enter description here](https://www.github.com/liuyong520/pic/raw/master/小书匠/1559543906788.png)
每次只扫描增量的日志数据，就能够极大提高效率，缩短数据不一致的时间窗口，如上图2-8流程所示：
- 写入正表DB1
- 第一步成功后，写入日志log1
- 写入反表DB2
- 第二步成功后，写入日志log2
当然，我们还是需要一个离线的扫描工具，不停的比对日志log1和日志log2，如果发现数据不一致，就进行补偿修复

**优点**：
- 虽比方法一复杂，但仍然是比较简单的
- 数据扫描效率高，只扫描增量数据

**缺点**：
- 线上服务略有修改（代价不高，多写了2条日志）
- 虽然比方法一更实时，但时效性还是不高，不一致窗口取决于扫描的周期
 
有没有实时检测一致性并进行修复的方法呢？

## 线上实时检测“消息对”法
![enter description here](https://www.github.com/liuyong520/pic/raw/master/小书匠/1559544318583.png)
这次不是写日志了，而是向消息总线发送消息，如上图2-7流程所示：
- 写入正表DB1
- 第一步成功后，发送消息msg1
- 写入反表DB2
- 第二步成功后，发送消息msg2
这次不是需要一个周期扫描的离线工具了，而是一个实时订阅消息的服务不停的收消息。

假设正常情况下，msg1和msg2的接收时间应该在3s以内，如果检测服务在收到msg1后没有收到msg2，就尝试检测数据的一致性，不一致时进行补偿修复。

**优点**：
- 效率高
- 实时性高

**缺点**：
- 方案比较复杂，上线引入了消息总线这个组件
- 线下多了一个订阅总线的检测服务
 
however，技术方案本身就是一个投入产出比的折衷，可以根据业务对一致性的需求程度决定使用哪一种方法。我曾经做过IM系统，好友关系链上亿，好友数据正反表的数据冗余，使用的就是方法二。

# 四，总结
互联网数据量大的业务场景，常常:
- 使用水平切分来降低单库数据量
- 使用数据冗余的反范式设计来满足不同维度的查询需求
冗余数据三种方案：
(1)服务同步双写法能够很容易的实现数据冗余
(2)为了降低时延，可以优化为服务异步双写法
(3)为了屏蔽“冗余数据”对服务带来的复杂性，可以优化为线下异步双写法
保证数据一致性的方案：
(1)最简单的方式，线下脚本扫全量数据比对
(2)提高效率的方式，线下脚本扫增量数据比对
(3)最实时的方式，线上检测“消息对”
