{"meta":{"title":"跛足的登山者","subtitle":null,"description":null,"author":"xxydliuy","url":"http://www.liuyong520.cn","root":"/"},"pages":[{"title":"toc","date":"2019-04-23T09:31:21.000Z","updated":"2019-04-27T08:01:14.825Z","comments":true,"path":"java.html","permalink":"http://www.liuyong520.cn/java.html","excerpt":"","text":"我媳妇是个老色鬼"},{"title":"关于","date":"2019-05-12T04:14:00.813Z","updated":"2019-05-12T04:14:00.813Z","comments":false,"path":"about/index.html","permalink":"http://www.liuyong520.cn/about/index.html","excerpt":"","text":"欢迎访问我的新博客，会陆续把之前的cnblog上的博客迁移过来，也会不定期更新！1234567891011121314151617181920212223242526272829303132/** * _ooOoo_ * o8888888o * 88\" . \"88 * (| -_- |) * O\\ = /O * ____/`---'\\____ * .' \\\\| |// `. * / \\\\||| : |||// \\ * / _||||| -:- |||||- \\ * | | \\\\\\ - /// | | * | \\_| ''\\---/'' | | * \\ .-\\__ `-` ___/-. / * ___`. .' /--.--\\ `. . __ * .\"\" '&lt; `.___\\_&lt;|&gt;_/___.' &gt;'\"\". * | | : `- \\`.;`\\ _ /`;.`/ - ` : | | * \\ \\ `-. \\_ __\\ /__ _/ .-` / / * ======`-.____`-.___\\_____/___.-`____.-'====== * `=---=' * ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ * 佛祖保佑 永无BUG * 佛曰: * 写字楼里写字间，写字间里程序员； * 程序人员写程序，又拿程序换酒钱。 * 酒醒只在网上坐，酒醉还来网下眠； * 酒醉酒醒日复日，网上网下年复年。 * 但愿老死电脑间，不愿鞠躬老板前； * 奔驰宝马贵者趣，公交自行程序员。 * 别人笑我忒疯癫，我笑自己命太贱； * 不见满街漂亮妹，哪个归得程序员？*/"},{"title":"分类","date":"2019-04-26T14:57:26.514Z","updated":"2019-04-26T14:57:26.514Z","comments":false,"path":"categories/index.html","permalink":"http://www.liuyong520.cn/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2019-04-26T15:28:00.106Z","updated":"2019-04-26T15:28:00.106Z","comments":false,"path":"books/index.html","permalink":"http://www.liuyong520.cn/books/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-04-26T15:24:46.349Z","updated":"2019-04-26T15:24:46.349Z","comments":false,"path":"tags/index.html","permalink":"http://www.liuyong520.cn/tags/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-04-26T15:44:12.763Z","updated":"2019-04-26T15:44:12.763Z","comments":true,"path":"links/index.html","permalink":"http://www.liuyong520.cn/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-04-26T11:51:41.915Z","updated":"2019-04-26T11:51:41.915Z","comments":false,"path":"repository/index.html","permalink":"http://www.liuyong520.cn/repository/index.html","excerpt":"","text":"qwqwwwwww"}],"posts":[{"title":"storm 的分组策略深入理解（-）","slug":"storm-groupping","date":"2019-05-10T13:37:40.000Z","updated":"2019-05-11T14:37:35.583Z","comments":true,"path":"2019/05/10/storm-groupping/","link":"","permalink":"http://www.liuyong520.cn/2019/05/10/storm-groupping/","excerpt":"","text":"storm的分组策略洗牌分组(Shuffle grouping): 随机分配元组到Bolt的某个任务上，这样保证同一个Bolt的每个任务都能够得到相同数量的元组。字段分组(Fields grouping): 按照指定的分组字段来进行流的分组。例如，流是用字段“user-id”来分组的，那有着相同“user-id”的元组就会分到同一个任务里，但是有不同“user-id”的元组就会分到不同的任务里。这是一种非常重要的分组方式，通过这种流分组方式，我们就可以做到让Storm产出的消息在这个”user-id”级别是严格有序的，这对一些对时序敏感的应用(例如，计费系统)是非常重要的。Partial Key grouping: 跟字段分组一样，流也是用指定的分组字段进行分组的，但是在多个下游Bolt之间是有负载均衡的，这样当输入数据有倾斜时可以更好的利用资源。这篇论文很好的解释了这是如何工作的，有哪些优势。All grouping: 流会复制给Bolt的所有任务。小心使用这种分组方式。在拓扑中，如果希望某类元祖发送到所有的下游消费者，就可以使用这种All grouping的流分组策略。Global grouping: 整个流会分配给Bolt的一个任务。具体一点，会分配给有最小ID的任务。不分组(None grouping): 说明不关心流是如何分组的。目前，None grouping等价于洗牌分组。Direct grouping：一种特殊的分组。对于这样分组的流，元组的生产者决定消费者的哪个任务会接收处理这个元组。只能在声明做直连的流(direct streams)上声明Direct groupings分组方式。只能通过使用emitDirect系列函数来吐元组给直连流。一个Bolt可以通过提供的TopologyContext来获得消费者的任务ID，也可以通过OutputCollector对象的emit函数(会返回元组被发送到的任务的ID)来跟踪消费者的任务ID。在ack的实现中，Spout有两个直连输入流，ack和ackFail，使用了这种直连分组的方式。Local or shuffle grouping：如果目标Bolt在同一个worker进程里有一个或多个任务，元组就会通过洗牌的方式分配到这些同一个进程内的任务里。否则，就跟普通的洗牌分组一样。这种方式的好处是可以提高拓扑的处理效率，因为worker内部通信就是进程内部通信了，相比拓扑间的进程间通信要高效的多。worker进程间通信是通过使用Netty来进行网络通信的。根据实例来分析分组策略common配置：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.sonly.strom&lt;/groupId&gt; &lt;artifactId&gt;strom-study&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;7&lt;/source&gt; &lt;target&gt;7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.sonly.storm.demo1.HelloToplogy&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;Shuffle groupingshuffle grouping的实例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.sonly.storm.demo1.grouppings.spout;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;import java.util.Random;import java.util.concurrent.atomic.AtomicInteger;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 20:27&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class WordSpout extends BaseRichSpout &#123; public static final Logger LOGGER = LoggerFactory.getLogger(WordSpout.class); //拓扑上下文 private TopologyContext context; private SpoutOutputCollector collector; private Map config; private AtomicInteger atomicInteger = new AtomicInteger(0); public void open(Map conf, TopologyContext topologyContext, SpoutOutputCollector collector) &#123; this.config = conf; this.context = topologyContext; this.collector = collector; LOGGER.warn(\"WordSpout-&gt;open:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\", this.hashCode(), Thread.currentThread().getId(), context.getThisTaskId()); &#125; public void nextTuple() &#123; String[] sentences = new String[]&#123;\"zhangsan\",\"zhangsan\",\"zhangsan\",\"zhangsan\",\"zhangsan\",\"zhangsan\",\"zhangsan\",\"zhangsan\",\"lisi\",\"lisi\"&#125;; int i = atomicInteger.get(); if(i&lt;10)&#123; atomicInteger.incrementAndGet(); final String sentence = sentences[i]; collector.emit(new Values(sentence)); LOGGER.warn(\"WordSpout-&gt;nextTuple:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;,Values:&#123;&#125;\", this.hashCode(), Thread.currentThread().getId(), context.getThisTaskId(), sentence); &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"sentence\")); &#125;&#125;bolt112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.sonly.storm.demo1.grouppings;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.Map;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:19&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class SheffleGroupingBolt extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(SheffleGroupingBolt.class); private TopologyContext context; private Map conf; private OutputCollector collector; private Map&lt;String,Integer&gt; counts = new HashMap(16); public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.conf=map; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"SheffleGroupingBolt-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getString(0); LOGGER.warn(\"SheffleGroupingBolt-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;,value:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId(),word); collector.emit(new Values(word)); collector.ack(tuple); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"bolt1\")); &#125;&#125;bolt12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.sonly.storm.demo1.grouppings;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:29&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class SheffleGrouppingBolt1 extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(SheffleGrouppingBolt1.class); private TopologyContext context; private Map conf; private OutputCollector collector; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.conf=map; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"SheffleGrouppingBolt1-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getStringByField(\"sentence\"); LOGGER.warn(\"SheffleGroupingBolt1-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;,value:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId(),word); collector.emit(new Values(word)); collector.ack(tuple); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"bolt\")); &#125;&#125;topology123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161package com.sonly.storm.demo1.grouppings;import com.sonly.storm.demo1.grouppings.spout.WordSpout;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.generated.AlreadyAliveException;import org.apache.storm.generated.AuthorizationException;import org.apache.storm.generated.InvalidTopologyException;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:55&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class ShuffleGroupingToplogy &#123; public static final Logger LOGGER = LoggerFactory.getLogger(ShuffleGroupingToplogy.class); //Topology Name //component prefix //workers //spout executor (parallelism_hint) //spout task size //bolt executor (parallelism_hint) //bolt task size public static void main(String[] args) throws InterruptedException &#123; TopologyBuilder builder = new TopologyBuilder(); Config conf = new Config(); conf.setDebug(true); if (args==null || args.length &lt; 7) &#123; conf.setNumWorkers(3); builder.setSpout(\"spout\", new WordSpout(), 4).setNumTasks(4); builder.setBolt(\"split-bolt\", new SheffleGrouppingBolt1(), 4).shuffleGrouping(\"spout\").setNumTasks(8); builder.setBolt(\"count-bolt\", new SheffleGroupingBolt(), 8).fieldsGrouping(\"split-bolt\", new Fields(\"word\")).setNumTasks(8); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"word-count\", conf, builder.createTopology()); Thread.sleep(10000); cluster.killTopology(\"word-count\"); cluster.shutdown(); &#125; else &#123; Options options = Options.builder(args); LOGGER.warn(\"The Topology Options &#123;&#125; is Submited \",options.toString()); conf.setNumWorkers(options.getWorkers()); builder.setSpout(options.getPrefix()+\"-spout\", new WordSpout(), options.getSpoutParallelismHint()).setNumTasks(options.getSpoutTaskSize()); builder.setBolt(\"bolt1\", new SheffleGrouppingBolt1(), options.getBoltParallelismHint()).shuffleGrouping(options.getPrefix()+\"-spout\").setNumTasks(options.getBoltTaskSize()); builder.setBolt(\"bolt\", new SheffleGroupingBolt(), options.getBoltParallelismHint()).shuffleGrouping(options.getPrefix()+\"-spout\").setNumTasks(options.getBoltTaskSize()); try &#123; StormSubmitter.submitTopologyWithProgressBar(options.getTopologyName(), conf, builder.createTopology()); LOGGER.warn(\"===========================================================\"); LOGGER.warn(\"The Topology &#123;&#125; is Submited \",options.getTopologyName()); LOGGER.warn(\"===========================================================\"); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static class Options&#123; private String topologyName; private String prefix; private Integer workers; private Integer spoutParallelismHint; private Integer spoutTaskSize; private Integer boltParallelismHint; private Integer boltTaskSize; public Options(String topologyName, String prefix, Integer workers, Integer spoutParallelismHint, Integer spoutTaskSize, Integer boltParallelismHint, Integer boltTaskSize) &#123; this.topologyName = topologyName; this.prefix = prefix; this.workers = workers; this.spoutParallelismHint = spoutParallelismHint; this.spoutTaskSize = spoutTaskSize; this.boltParallelismHint = boltParallelismHint; this.boltTaskSize = boltTaskSize; &#125; public static Options builder(String[] args)&#123; return new Options(args[0],args[1],Integer.parseInt(args[2]) ,Integer.parseInt(args[3]),Integer.parseInt(args[4]),Integer.parseInt(args[5]),Integer.parseInt(args[6]) ); &#125; public String getTopologyName() &#123; return topologyName; &#125; public void setTopologyName(String topologyName) &#123; this.topologyName = topologyName; &#125; public String getPrefix() &#123; return prefix; &#125; public void setPrefix(String prefix) &#123; this.prefix = prefix; &#125; public Integer getWorkers() &#123; return workers; &#125; public void setWorkers(Integer workers) &#123; this.workers = workers; &#125; public Integer getSpoutParallelismHint() &#123; return spoutParallelismHint; &#125; public void setSpoutParallelismHint(Integer spoutParallelismHint) &#123; this.spoutParallelismHint = spoutParallelismHint; &#125; public Integer getSpoutTaskSize() &#123; return spoutTaskSize; &#125; public void setSpoutTaskSize(Integer spoutTaskSize) &#123; this.spoutTaskSize = spoutTaskSize; &#125; public Integer getBoltParallelismHint() &#123; return boltParallelismHint; &#125; public void setBoltParallelismHint(Integer boltParallelismHint) &#123; this.boltParallelismHint = boltParallelismHint; &#125; public Integer getBoltTaskSize() &#123; return boltTaskSize; &#125; public void setBoltTaskSize(Integer boltTaskSize) &#123; this.boltTaskSize = boltTaskSize; &#125; @Override public String toString() &#123; return \"Options&#123;\" + \"topologyName='\" + topologyName + '\\'' + \", prefix='\" + prefix + '\\'' + \", workers=\" + workers + \", spoutParallelismHint=\" + spoutParallelismHint + \", spoutTaskSize=\" + spoutTaskSize + \", boltParallelismHint=\" + boltParallelismHint + \", boltTaskSize=\" + boltTaskSize + '&#125;'; &#125; &#125;&#125;mvn package 打包，上传到storm服务器ShuffleGrouping 样例分析1)样例11.执行：1storm jar strom-study-1.0-SNAPSHOT-jar-with-dependencies.jar com.sonly.storm.demo1.grouppings.ShuffleGroupingToplogy ShuffleGrouping ShuffleGrouping 1 2 1 2 12.参数：topologyName=’ShuffleGrouping’, prefix=’ShuffleGrouping’, workers=1, spoutParallelismHint=2, spoutTaskSize=1, boltParallelismHint=2, boltTaskSize=13.拓扑图：一个spout接了两个bolt4.查看一下这个bolt分布情况：5.进入服务器去看每一个bolt的日志123456789102019-05-07 18:09:13.109 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.110 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.110 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.111 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.112 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.115 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.116 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.117 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 18:09:13.118 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:lisi2019-05-07 18:09:13.119 c.s.s.d.g.SheffleGrouppingBolt1 Thread-11-bolt1-executor[5 5] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:1393282516-&gt;ThreadId:45,TaskId:5,value:lisi6.进入另外一个bolt的日志 10条信息被处理了123456789102019-05-07 18:09:00.791 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.793 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.794 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.795 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.795 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.796 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.797 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.805 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:zhangsan2019-05-07 18:09:00.805 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:lisi2019-05-07 18:09:00.806 c.s.s.d.g.SheffleGroupingBolt Thread-9-bolt-executor[4 4] [WARN] SheffleGroupingBolt-&gt;execute:hashcode:1430296959-&gt;ThreadId:43,TaskId:4,value:lisi也是一样10条被处理了总结：对于spout直接对接两个bolt，sheffgrouping 分组不会随机给两个bolt分配消息，而是全量发给两个BOlT2）样例21.修改一下参数看一下：topologyName=’ShuffleGrouping1’, prefix=’ShuffleGrouping1’, workers=2, spoutParallelismHint=1, spoutTaskSize=2, boltParallelismHint=2, boltTaskSize=2总共4个bolt，两个spout，总共发送了40条消息，spout产生消息20条。transfer了40次。看看4个bolt的消息分配的情况。因为只有两个worker所以会有两个bolt在同一个work上，日志会打在一起，但是从名字可以可以区分开来，同样每个bolt都是10条。2.修改拓扑结构为：3.修改代码：bolt1String word = tuple.getStringByField(\"bolt\");topoloy：12 builder.setBolt(\"bolt1\", new SheffleGrouppingBolt1(), 1).shuffleGrouping(options.getPrefix()+\"-spout\");builder.setBolt(\"bolt\", new SheffleGroupingBolt(), options.getBoltParallelismHint()).shuffleGrouping(\"bolt1\").setNumTasks(options.getBoltTaskSize());4.参数：topologyName=’ShuffleGrouping2’, prefix=’ShuffleGrouping2’, workers=2, spoutParallelismHint=1, spoutTaskSize=1, boltParallelismHint=2, boltTaskSize=25.查看日志：k8s-n2 这个节点只有bolt bolt1这个节点在k8s-n3上12345678[root@k8s-n2 6706]# grep &quot;SheffleGroupingBolt-&gt;execute&quot; worker.log |wc -l3[root@k8s-n2 6706]# grep &quot;SheffleGroupingBolt1-&gt;execute&quot; worker.log |wc -l0[root@k8s-n3 6706]# grep &quot;SheffleGroupingBolt-&gt;execute&quot; worker.log |wc -l7[root@k8s-n3 6706]# grep &quot;SheffleGroupingBolt1-&gt;execute&quot; worker.log |wc -l10可以看出来bolt1-&gt;bolt这条线上的数据被随机分配了一个三条一个两条。总结：对于bolt 连接bolt的shuffingGrouping，消息是随机分配到多个bolt上面的Fields groupingFields grouping 的实例代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130public class FeildGroupingToplogy &#123; public static final Logger LOGGER = LoggerFactory.getLogger(FeildGroupingToplogy.class); //Topology Name //component prefix //workers //spout executor (parallelism_hint) //spout task size //bolt executor (parallelism_hint) //bolt task size public static void main(String[] args) throws InterruptedException &#123; TopologyBuilder builder = new TopologyBuilder(); Config conf = new Config(); conf.setDebug(true); Options options = Options.builder(args); LOGGER.warn(\"The Topology Options &#123;&#125; is Submited \", options.toString()); conf.setNumWorkers(options.getWorkers()); String spoutName = options.getPrefix() + \"-spout\"; builder.setSpout(spoutName, new WordSpout(), options.getSpoutParallelismHint()).setNumTasks(options.getSpoutTaskSize()); builder.setBolt(options.getPrefix() + \"bolt1\", new FieldGrouppingBolt1(), options.getBoltParallelismHint()).fieldsGrouping(spoutName, new Fields(\"sentence\")).setNumTasks(options.getBoltTaskSize()); builder.setBolt(options.getPrefix() + \"bolt\", new FieldGroupingBolt(), options.getBoltParallelismHint()).fieldsGrouping(spoutName, new Fields(\"sentence\")).setNumTasks(options.getBoltTaskSize());// builder.setBolt(\"bolt1\", new FieldGrouppingBolt1(), 1).shuffleGrouping(options.getPrefix()+\"-spout\");// builder.setBolt(\"bolt\", new FieldGroupingBolt(), options.getBoltParallelismHint()).fieldsGrouping(\"bolt1\",new Fields(\"bolt\")).setNumTasks(options.getBoltTaskSize()); try &#123; StormSubmitter.submitTopologyWithProgressBar(options.getTopologyName(), conf, builder.createTopology()); LOGGER.warn(\"===========================================================\"); LOGGER.warn(\"The Topology &#123;&#125; is Submited \", options.getTopologyName()); LOGGER.warn(\"===========================================================\"); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; public static class Options &#123; private String topologyName; private String prefix; private Integer workers; private Integer spoutParallelismHint; private Integer spoutTaskSize; private Integer boltParallelismHint; private Integer boltTaskSize; public Options(String topologyName, String prefix, Integer workers, Integer spoutParallelismHint, Integer spoutTaskSize, Integer boltParallelismHint, Integer boltTaskSize) &#123; this.topologyName = topologyName; this.prefix = prefix; this.workers = workers; this.spoutParallelismHint = spoutParallelismHint; this.spoutTaskSize = spoutTaskSize; this.boltParallelismHint = boltParallelismHint; this.boltTaskSize = boltTaskSize; &#125; public static Options builder(String[] args) &#123; return new Options(args[0], args[1], Integer.parseInt(args[2]) , Integer.parseInt(args[3]), Integer.parseInt(args[4]), Integer.parseInt(args[5]), Integer.parseInt(args[6]) ); &#125; public String getTopologyName() &#123; return topologyName; &#125; public void setTopologyName(String topologyName) &#123; this.topologyName = topologyName; &#125; public String getPrefix() &#123; return prefix; &#125; public void setPrefix(String prefix) &#123; this.prefix = prefix; &#125; public Integer getWorkers() &#123; return workers; &#125; public void setWorkers(Integer workers) &#123; this.workers = workers; &#125; public Integer getSpoutParallelismHint() &#123; return spoutParallelismHint; &#125; public void setSpoutParallelismHint(Integer spoutParallelismHint) &#123; this.spoutParallelismHint = spoutParallelismHint; &#125; public Integer getSpoutTaskSize() &#123; return spoutTaskSize; &#125; public void setSpoutTaskSize(Integer spoutTaskSize) &#123; this.spoutTaskSize = spoutTaskSize; &#125; public Integer getBoltParallelismHint() &#123; return boltParallelismHint; &#125; public void setBoltParallelismHint(Integer boltParallelismHint) &#123; this.boltParallelismHint = boltParallelismHint; &#125; public Integer getBoltTaskSize() &#123; return boltTaskSize; &#125; public void setBoltTaskSize(Integer boltTaskSize) &#123; this.boltTaskSize = boltTaskSize; &#125; @Override public String toString() &#123; return \"Options&#123;\" + \"topologyName='\" + topologyName + '\\'' + \", prefix='\" + prefix + '\\'' + \", workers=\" + workers + \", spoutParallelismHint=\" + spoutParallelismHint + \", spoutTaskSize=\" + spoutTaskSize + \", boltParallelismHint=\" + boltParallelismHint + \", boltTaskSize=\" + boltTaskSize + '&#125;'; &#125; &#125;&#125;12345678910111213141516171819202122232425public class FieldGroupingBolt extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(FieldGroupingBolt.class); private TopologyContext context; private Map conf; private OutputCollector collector; private Map&lt;String,Integer&gt; counts = new HashMap(16); public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.conf=map; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"FieldGroupingBolt-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getStringByField(\"bolt\"); LOGGER.warn(\"FieldGroupingBolt-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;,value:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId(),word); collector.emit(new Values(word)); collector.ack(tuple); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"bolt1\")); &#125;&#125;123456789101112131415161718192021public class FieldGrouppingBolt1 extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(FieldGrouppingBolt1.class); private TopologyContext context; private OutputCollector collector; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"FieldGrouppingBolt1-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; String word = tuple.getStringByField(\"sentence\"); LOGGER.warn(\"SheffleGroupingBolt1-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;,value:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId(),word); collector.emit(new Values(word)); collector.ack(tuple); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"bolt\")); &#125;&#125;2.打包上传到服务器3.执行：1storm jar strom-study-1.0-SNAPSHOT-jar-with-dependencies.jar com.sonly.storm.demo1.grouppings.fieldgrouping.FeildGroupingToplogy FieldGrouping1 FieldGrouping1 2 1 1 2 24.参数topologyName=’FieldGrouping1’, prefix=’FieldGrouping1’, workers=2, spoutParallelismHint=1, spoutTaskSize=1, boltParallelismHint=2, boltTaskSize=25。拓扑图：6.并发度以及组件分布图：同样看图可以看到消息被发送了20次，但是被transfer40次。这是因为spout对bolt，对消息进行了复制，全量发送到了每个bolt，所以每个bolt都会有10条消息。总结：和sheffleGrouping 一样，spout-&gt;bolt是全量广播发送，每个bolt都会spout的全量消息。样例21.修改拓扑的代码123456789101112131415161718192021222324public static void main(String[] args) throws InterruptedException &#123; TopologyBuilder builder = new TopologyBuilder(); Config conf = new Config(); conf.setDebug(true); Options options = Options.builder(args); LOGGER.warn(&quot;The Topology Options &#123;&#125; is Submited &quot;, options.toString()); conf.setNumWorkers(options.getWorkers()); String spoutName = options.getPrefix() + &quot;-spout&quot;; builder.setSpout(spoutName, new WordSpout(), options.getSpoutParallelismHint()).setNumTasks(options.getSpoutTaskSize());// builder.setBolt(options.getPrefix() + &quot;bolt1&quot;, new FieldGrouppingBolt1(), options.getBoltParallelismHint()).fieldsGrouping(spoutName, new Fields(&quot;sentence&quot;)).setNumTasks(options.getBoltTaskSize());// builder.setBolt(options.getPrefix() + &quot;bolt&quot;, new FieldGroupingBolt(), options.getBoltParallelismHint()).fieldsGrouping(spoutName, new Fields(&quot;sentence&quot;)).setNumTasks(options.getBoltTaskSize()); builder.setBolt(&quot;bolt1&quot;, new FieldGrouppingBolt1(), 1).fieldsGrouping(spoutName, new Fields(&quot;sentence&quot;)); builder.setBolt(&quot;bolt&quot;, new FieldGroupingBolt(), options.getBoltParallelismHint()).fieldsGrouping(&quot;bolt1&quot;,new Fields(&quot;bolt&quot;)).setNumTasks(options.getBoltTaskSize()); try &#123; StormSubmitter.submitTopologyWithProgressBar(options.getTopologyName(), conf, builder.createTopology()); LOGGER.warn(&quot;===========================================================&quot;); LOGGER.warn(&quot;The Topology &#123;&#125; is Submited &quot;, options.getTopologyName()); LOGGER.warn(&quot;===========================================================&quot;); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125;FieldGrouping 样例分析1）样例12.将上面代码打包上传服务器执行命令1storm jar strom-study-1.0-SNAPSHOT-jar-with-dependencies.jar com.sonly.storm.demo1.grouppings.fieldgrouping.FeildGroupingToplogy FieldGrouping2 FieldGrouping2 2 1 1 2 23.参数topologyName=’FieldGrouping2’, prefix=’FieldGrouping2’, workers=2, spoutParallelismHint=1, spoutTaskSize=1, boltParallelismHint=2, boltTaskSize=4.拓扑图：6.并发度以及组件分布图：7.根据分布情况检查各个work的日志查看消息的发送情况k8s-n3节点上：1234[root@k8s-n3 6701]# grep &quot;SheffleGroupingBolt1-&gt;execute&quot; worker.log|wc -l10[root@k8s-n3 6701]# grep &quot;FieldGroupingBolt-&gt;execute&quot; worker.log|wc -l2k8s-n2节点：1234[root@k8s-n2 6701]# grep &quot;SheffleGroupingBolt1-&gt;execute&quot; worker.log|wc -l0[root@k8s-n2 6701]# grep &quot;FieldGroupingBolt-&gt;execute&quot; worker.log|wc -l8再看一下详情如何k8s-n3：bolt1有10条消息应为bolt只有一个所以Fied分组是不会生效的。1234567891011[root@k8s-n3 6701]# grep &quot;SheffleGroupingBolt1-&gt;execute&quot; worker.log2019-05-07 21:59:35.805 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.810 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.810 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.811 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.811 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.812 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.813 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.814 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:zhangsan2019-05-07 21:59:35.815 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:lisi2019-05-07 21:59:35.838 c.s.s.d.g.f.FieldGrouppingBolt1 Thread-7-bolt1-executor[6 6] [WARN] SheffleGroupingBolt1-&gt;execute:hashcode:107880849-&gt;ThreadId:41,TaskId:6,value:lisik8s-n3:bolt 有两个实例，按照field分组。里面有两条消息。都是lisi123[root@k8s-n3 6701]# grep &quot;FieldGroupingBolt-&gt;execute&quot; worker.log2019-05-07 21:59:35.855 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[4 4] [WARN] FieldGroupingBolt-&gt;execute:hashcode:281792799-&gt;ThreadId:45,TaskId:4,value:lisi2019-05-07 21:59:35.856 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[4 4] [WARN] FieldGroupingBolt-&gt;execute:hashcode:281792799-&gt;ThreadId:45,TaskId:4,value:lisik8s-n2: bolt 应该就是8条消息，验证一下123456789[root@k8s-n2 6701]# grep &quot;FieldGroupingBolt-&gt;execute&quot; worker.log2019-05-07 21:59:48.315 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 21:59:48.317 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 21:59:48.317 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 21:59:48.318 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 21:59:48.318 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 21:59:48.319 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 21:59:48.319 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan2019-05-07 21:59:48.320 c.s.s.d.g.f.FieldGroupingBolt Thread-11-bolt-executor[5 5] [WARN] FieldGroupingBolt-&gt;execute:hashcode:1858735164-&gt;ThreadId:45,TaskId:5,value:zhangsan总结：bolt-&gt;bolt节点时，feild分组会按照field字段的key值进行分组，key相同的会被分配到一个bolt里面。如果执行1storm jar strom-study-1.0-SNAPSHOT-jar-with-dependencies.jar com.sonly.storm.demo1.grouppings.fieldgrouping.FeildGroupingToplogy FieldGrouping3 FieldGrouping3 4 1 1 4 4bolt 的分配情况是什么样子？这个留给大家去思考一下。例子中按照field分组后只有两种数据，但是这两种数据要分配给4个bolt，那这个是怎么分配的？我将在下一篇博客里揭晓答案！下一篇我会继续分析这个分组策略，我会把我在学习这个storm的时候当时的自己思考的一个过程展现给大家，如果有什么错误的，或者没有讲清楚的地方，欢迎大家给我留言，咱们可以一起交流讨论。","categories":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/categories/storm/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/tags/storm/"}]},{"title":"理解storm并发度","slug":"storm-parallelism","date":"2019-05-10T03:46:25.000Z","updated":"2019-05-11T00:36:38.975Z","comments":true,"path":"2019/05/10/storm-parallelism/","link":"","permalink":"http://www.liuyong520.cn/2019/05/10/storm-parallelism/","excerpt":"","text":"什么是storm的并发度一个topology（拓扑）在storm集群上最总是以executor和task的形式运行在suppervisor管理的worker节点上。而worker进程都是运行在jvm虚拟机上面的，每个拓扑都会被拆开多个组件分布式的运行在worker节点上。1.worker2.executor3.task这三个简单关系图：一个worker工作进程运行一个拓扑的子集（其实就是拓扑的组件），每个组件的都会以executor（线程）在worker进程上执行，一个worker进程可以同时运行多个拓扑的组件也就是线程。一个executor线程可以运行同一个组件的一个或者多个taskstask是实际处理数据的执行者，每一个spout或者bolt会在集群上执行很多个task。在拓扑的生命周期内拓扑结构相同的拓扑的组件任务task数量总是相同的。但是每个组件的执行的线程（executor）数是可以变化的。这就意味着以下条件总是成立的：#threads ≤ #tasks 也就是task的数量总是大于线程数，一般情况下，任务task的数量往往设置成和线程（executor）的数量一致，这样，每个线程执行一个task。在storm拓扑的并发度其实就是集群上拓扑组件在集群上运行的executor（线程）的数量。如何设置拓扑的并发度“并行度”如何配置？其实不仅仅是设置executor线程的数量,同时也要从worker工作进程和task任务的数量的方面考虑。可以用以下几种方式配置并发度：1.通过storm的配置文件配置。storm配置文件的加载优先级是：defaults.yaml &lt; storm.yaml &lt; topology-specific configuration &lt; internal component-specific configuration &lt; external component-specific configuration.工作进程数描述：为群集中的计算机上的拓扑创建多少个工作进程。配置选项：TOPOLOGY_WORKERS如何设置代码（示例）：配置＃setNumWorkers执行者数（线程数）描述：每个组件生成多少个执行程序。配置选项：无（将parallelism_hint参数传递给setSpout或setBolt）如何设置代码（示例）：TopologyBuilder＃setSpout（）TopologyBuilder＃setBolt（）请注意，从Storm 0.8开始，parallelism_hint参数现在指定该螺栓的执行者的初始数量（不是任务！）。任务数量描述：每个组件创建多少个任务。配置选项：TOPOLOGY_TASKS如何设置代码（示例）：ComponentConfigurationDeclarer＃setNumTasks（）以下是在实践中显示这些设置的示例代码段：1234567891011121314151617181920212223242526272829topologyBuilder.setBolt(\"green-bolt\", new GreenBolt(), 2) .setNumTasks(4) .shuffleGrouping(\"blue-spout\");``` 在上面的代码中，我们配置了Storm来运行GreenBolt带有初始数量为两个执行器和四个相关任务的bolt 。Storm将为每个执行程序（线程）运行两个任务。如果您没有明确配置任务数，Storm将默认运行每个执行程序一个任务。#官方例子下图显示了简单拓扑在操作中的外观。拓扑结构由三个部分组成：一个叫做spout BlueSpout，两个叫做GreenBolt和YellowBolt。组件被链接，以便BlueSpout将其输出发送到GreenBolt，然后将其自己的输出发送到YellowBolt。![官方图](https://www.github.com/liuyong520/pic/raw/master/小书匠/1557460241883.png)在GreenBolt被配置为每代码段以上而BlueSpout和YellowBolt仅设置并行提示（执行人数）。这是相关代码：```javaConfig conf = new Config();conf.setNumWorkers(2); // use two worker processestopologyBuilder.setSpout(\"blue-spout\", new BlueSpout(), 2); // set parallelism hint to 2topologyBuilder.setBolt(\"green-bolt\", new GreenBolt(), 2) .setNumTasks(4) .shuffleGrouping(\"blue-spout\");topologyBuilder.setBolt(\"yellow-bolt\", new YellowBolt(), 6) .shuffleGrouping(\"green-bolt\");StormSubmitter.submitTopology( \"mytopology\", conf, topologyBuilder.createTopology() );当然，Storm附带了额外的配置设置来控制拓扑的并行性，包括：TOPOLOGY_MAX_TASK_PARALLELISM：此设置为可以为单个组件生成的执行程序数量设置上限。它通常在测试期间用于限制在本地模式下运行拓扑时产生的线程数。您可以通过例如Config＃setMaxTaskParallelism（）设置此选项。从实际运行的拓扑的角度理解storm的并发度自己写一个拓扑实现一个可以设置worker数量，设置spout 、bolt 的Parallelism Hint的拓扑然后打包上传到storm集群运行。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146package com.sonly.storm.demo1;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.generated.AlreadyAliveException;import org.apache.storm.generated.AuthorizationException;import org.apache.storm.generated.InvalidTopologyException;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)HelloToplogy&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:55&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class HelloToplogy &#123; public static final Logger LOGGER = LoggerFactory.getLogger(HelloToplogy.class); //Topology Name //component prefix //workers //spout executor (parallelism_hint) //spout task size //bolt executor (parallelism_hint) //bolt task size public static void main(String[] args) throws InterruptedException &#123; TopologyBuilder builder = new TopologyBuilder(); Config conf = new Config(); conf.setDebug(true); if (args==null || args.length &lt; 7) &#123; conf.setNumWorkers(3); builder.setSpout(\"spout\", new HellowordSpout(), 4).setNumTasks(4); builder.setBolt(\"split-bolt\", new SplitBolt(), 4).shuffleGrouping(\"spout\").setNumTasks(8); builder.setBolt(\"count-bolt\", new HellowordBolt(), 8).fieldsGrouping(\"split-bolt\", new Fields(\"word\")).setNumTasks(8); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"word-count\", conf, builder.createTopology()); Thread.sleep(10000); cluster.killTopology(\"word-count\"); cluster.shutdown(); &#125; else &#123; Options options = Options.builder(args); conf.setNumWorkers(options.getWorkers()); builder.setSpout(options.getPrefix()+\"-spout\", new HellowordSpout(), options.getSpoutParallelismHint()).setNumTasks(options.getSpoutTaskSize()); builder.setBolt(options.getPrefix()+\"-split-bolt\", new SplitBolt(), options.getBoltParallelismHint()).shuffleGrouping(options.getPrefix()+\"-spout\").setNumTasks(options.getBoltTaskSize()); builder.setBolt(options.getPrefix()+\"-count-bolt\", new HellowordBolt(), options.getBoltParallelismHint()).fieldsGrouping(options.getPrefix()+\"-split-bolt\", new Fields(\"word\")).setNumTasks(options.getBoltTaskSize()); try &#123; StormSubmitter.submitTopologyWithProgressBar(options.getTopologyName(), conf, builder.createTopology()); LOGGER.warn(\"===========================================================\"); LOGGER.warn(\"The Topology &#123;&#125; is Submited \",options.getTopologyName()); LOGGER.warn(\"===========================================================\"); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static class Options&#123; private String topologyName; private String prefix; private Integer workers; private Integer spoutParallelismHint; private Integer spoutTaskSize; private Integer boltParallelismHint; private Integer boltTaskSize; public Options(String topologyName, String prefix, Integer workers, Integer spoutParallelismHint, Integer spoutTaskSize, Integer boltParallelismHint, Integer boltTaskSize) &#123; this.topologyName = topologyName; this.prefix = prefix; this.workers = workers; this.spoutParallelismHint = spoutParallelismHint; this.spoutTaskSize = spoutTaskSize; this.boltParallelismHint = boltParallelismHint; this.boltTaskSize = boltTaskSize; &#125; public static Options builder(String[] args)&#123; return new Options(args[0],args[1],Integer.parseInt(args[2]) ,Integer.parseInt(args[3]),Integer.parseInt(args[4]),Integer.parseInt(args[5]),Integer.parseInt(args[6]) ); &#125; public String getTopologyName() &#123; return topologyName; &#125; public void setTopologyName(String topologyName) &#123; this.topologyName = topologyName; &#125; public String getPrefix() &#123; return prefix; &#125; public void setPrefix(String prefix) &#123; this.prefix = prefix; &#125; public Integer getWorkers() &#123; return workers; &#125; public void setWorkers(Integer workers) &#123; this.workers = workers; &#125; public Integer getSpoutParallelismHint() &#123; return spoutParallelismHint; &#125; public void setSpoutParallelismHint(Integer spoutParallelismHint) &#123; this.spoutParallelismHint = spoutParallelismHint; &#125; public Integer getSpoutTaskSize() &#123; return spoutTaskSize; &#125; public void setSpoutTaskSize(Integer spoutTaskSize) &#123; this.spoutTaskSize = spoutTaskSize; &#125; public Integer getBoltParallelismHint() &#123; return boltParallelismHint; &#125; public void setBoltParallelismHint(Integer boltParallelismHint) &#123; this.boltParallelismHint = boltParallelismHint; &#125; public Integer getBoltTaskSize() &#123; return boltTaskSize; &#125; public void setBoltTaskSize(Integer boltTaskSize) &#123; this.boltTaskSize = boltTaskSize; &#125; &#125;&#125;spout 类：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.sonly.storm.demo1;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Currency;import java.util.Map;import java.util.Random;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;HellowordSpout&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 20:27&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class HellowordSpout extends BaseRichSpout &#123; public static final Logger LOGGER = LoggerFactory.getLogger(HellowordSpout.class); //拓扑上下文 private TopologyContext context; private SpoutOutputCollector collector; private Map config; private Random random; public void open(Map conf, TopologyContext topologyContext, SpoutOutputCollector collector) &#123; this.config = conf; this.context = topologyContext; this.collector = collector; this.random = new Random(); LOGGER.warn(\"HellowordSpout-&gt;open:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void nextTuple() &#123; String[] sentences = new String[]&#123;\"hello world !\", \"hello Storm !\", \"hello apache flink !\", \"hello apache kafka stream !\", \"hello apache spark !\"&#125;; final String sentence = sentences[random.nextInt(sentences.length)]; collector.emit(new Values(sentence)); LOGGER.warn(\"HellowordSpout-&gt;nextTuple:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;,Values:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId(),sentence); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"sentence\")); &#125; @Override public void close() &#123; LOGGER.warn(\"HellowordSpout-&gt;close:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); super.close(); &#125;&#125;实现两个bolt一个用来统计单词出现个数，一个用来拆分语句。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.sonly.storm.demo1;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.Map;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:19&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class HellowordBolt extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(HellowordBolt.class); private TopologyContext context; private Map conf; private OutputCollector collector; private Map&lt;String,Integer&gt; counts = new HashMap(16); public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.conf=map; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"HellowordBolt-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; LOGGER.warn(\"HellowordBolt-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); String word = tuple.getString(0); Integer count = counts.get(word); if (count == null) count = 0; count++; counts.put(word, count); collector.emit(new Values(word, count)); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"word\", \"count\")); &#125;&#125;12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.sonly.storm.demo1;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:29&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class SplitBolt extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(SplitBolt.class); private TopologyContext context; private Map conf; private OutputCollector collector; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.conf=map; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"SplitBolt-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; String words = tuple.getStringByField(\"sentence\"); String[] contents = words.split(\" +\"); for (String content : contents) &#123; collector.emit(new Values(content)); collector.ack(tuple); &#125; LOGGER.warn(\"SplitBolt-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"word\")); &#125;&#125;local模式启动运行在pom文件中添加打包插件12345678910111213141516171819202122&lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.sonly.storm.demo1.HelloToplogy&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;同时修改dependency 的scope为provide1&lt;scope&gt;provide&lt;/scope&gt;原因是服务器上storm相关包都已经存在了，防止重复打包导致冲突。1234567//Topology Name//component prefix//workers//spout executor (parallelism_hint)//spout task size//bolt executor (parallelism_hint)//bolt task size在storm集群提交拓扑修改日志级别修改worker的工作进程的日志级别，修改成只输出warn日志，避免其他日志对我的干扰。进入${your_storm_path}/log4j2/目录修改worker.xml文件。先把worker.xml备份把Info级别改成warn1$ cp worker.xml worker.xml.bak修改成：1234567891011121314151617&lt;loggers&gt; &lt;root level=&quot;warn&quot;&gt; &lt;!-- We log everything --&gt; &lt;appender-ref ref=&quot;A1&quot;/&gt; &lt;appender-ref ref=&quot;syslog&quot;/&gt; &lt;/root&gt; &lt;Logger name=&quot;org.apache.storm.metric.LoggingMetricsConsumer&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;METRICS&quot;/&gt; &lt;/Logger&gt; &lt;Logger name=&quot;STDERR&quot; level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STDERR&quot;/&gt; &lt;appender-ref ref=&quot;syslog&quot;/&gt; &lt;/Logger&gt; &lt;Logger name=&quot;STDOUT&quot; level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;appender-ref ref=&quot;syslog&quot;/&gt; &lt;/Logger&gt;&lt;/loggers&gt;同步到另外两台supervisor的工作节点服务器。为了跟清晰的理解并发度，我会通过这个demo 拓扑，修改参数观察stormUI的exector数量和tasks数量。参数说明1234567// topologyName=&apos;count&apos; ## Topology Name 拓扑的名字// prefix=&apos;tp1&apos; ## component prefix 即为每个spout，bolt的前缀名称// workers=1 ## worker number 即为工作进程jvm数量// spoutParallelismHint=2 ## spout executor (parallelism_hint) 即spout的线程数量// spoutTaskSize=1 ## spout task size 即spout的运行实例数// boltParallelismHint=2 ## bolt executor (parallelism_hint) 即bolt的线程数量// boltTaskSize=1 ##bolt task size 即bolt的运行实例数根据样例分析理解storm的并发度例子1执行：1storm jar storm-demo1.jar com.sonly.storm.demo1.HelloToplogy tp1 tp1 1 2 0 2 0参数详情：1&#123;topologyName=&apos;tp1&apos;, prefix=&apos;tp1&apos;, workers=1, spoutParallelismHint=2, spoutTaskSize=0, boltParallelismHint=2, boltTaskSize=0&#125;这时候task都被设置成0了。如下图：excutors为1，task为1。接着往下看：此时我们的bolt的task都被设置0了，所以我们是没有创建spout，bolt的，但是你会发现一个_acker的bolt，这是storm的acker机制，storm自己给我们创建的bolt，并且每一个worker都会必须有一个_acker的bolt，如果我们没有取消ack机制的话。所以worker上只用了一个excutor来跑这个_acker的bolt。例子21storm jar storm-demo1.jar com.sonly.storm.demo1.HelloToplogy tp2 tp2 1 2 1 2 1参数详情：1&#123;topologyName=&apos;tp2&apos;, prefix=&apos;tp2&apos;, workers=1, spoutParallelismHint=2, spoutTaskSize=1, boltParallelismHint=2, boltTaskSize=1&#125;此时 task的值都被设置成1了。如下图：excutors为4，task为4。接着看一下spout，bolt 以及组件的分布情况见下图：此时已经我们的有tp2-spout 一个spout，除了系统的acker 还有我们自己创建的两个bolt。因为只有一个worker所以全部分布在一个worker里面。尽管我们设置了spout的线程数为2，bolt的线程数为2，但是task都被设置成1，只有一个任务需要被两个excutor执行，所以有一个线程实际上是没有任务执行的。所以线程数，就是这几个task的值的和，一个spout，两个自己的创建的bolt以及acker的task数量的和。例子31storm jar storm-demo1.jar com.sonly.storm.demo1.HelloToplogy tp3 tp3 2 2 1 2 1参数详情：1&#123;topologyName=&apos;tp3&apos;, prefix=&apos;tp3&apos;, workers=2, spoutParallelismHint=2, spoutTaskSize=1, boltParallelismHint=2, boltTaskSize=1&#125;此时worker已经被设置成2了，如下图：executor为5，task为5.接着看一下spout，bolt以及组件的分布情况如下图：此时，task任务数依然是1，spout和bolt都是1份，acker每个worker都必须有一份的，所以，executor的数就是task实例数也就是：一个spout 两个系统acker bolt，和两个我们自己的bolt。也就是5.这个5个task不均匀的分配到了两个worker进程上。例子41storm jar storm-demo1.jar com.sonly.storm.demo1.HelloToplogy tp4 tp4 2 2 2 2 2参数详情：1&#123;topologyName=&apos;tp4&apos;, prefix=&apos;tp4&apos;, workers=2, spoutParallelismHint=2, spoutTaskSize=2, boltParallelismHint=2, boltTaskSize=2&#125;此时参数已经taks 数量被设置成2了，如下图：executor为8，task为8.再看一下spout，bolt的分布情况：如下图：此时，我们task都被设置成了2，那spout实例和bolt的实例都是2，也就是2+2+2=6 这个是我们自己的创建的task，再加上acker两个task，所以task参数就是8.而这里设置时executor也是8个 被均分到两个worker上面。例子51storm jar storm-demo1.jar com.sonly.storm.demo1.HelloToplogy tp5 tp5 2 2 4 2 4参数详情：1&#123;topologyName=&apos;tp5&apos;, prefix=&apos;tp5&apos;, workers=2, spoutParallelismHint=2, spoutTaskSize=4, boltParallelismHint=2, boltTaskSize=4&#125;此时task设置成4，excutor设置成2，那这样的话，一个excutor会跑两个task ，executor=8,task=14 如下图：继续看一下spout和bolt的分布情况：此时task设置成4，executor是2，那就是bolt和spout实例就是 4+4+4=12 再加上两个worker的Acker就是14个task。exector 是bolt的设置的值2+2+2=6个再加上两个acker的值，就是8个。同时，一个executor执行了两个task。8个executor平均分配到两个worker上面了。总结exector和task的值，和拓扑结构有关系，拓扑中的spout 和bolt设置的parallelism_hint都会影响到exector和task的数量。task和exectuor之间的关系在设置上就已经确定了，最好exector和task之间，task 的数量最好设置成executor的倍数，这样每个executor执行的task才是一样的。说到这里，相信大家对并发度，有了比较清晰的理解。","categories":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/categories/storm/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/tags/storm/"}]},{"title":"storm 基本知识","slug":"storm-base","date":"2019-05-02T14:00:40.000Z","updated":"2019-05-09T16:02:25.209Z","comments":true,"path":"2019/05/02/storm-base/","link":"","permalink":"http://www.liuyong520.cn/2019/05/02/storm-base/","excerpt":"","text":"引言介绍storm之前，我先抛出这两个问题：1.实时计算需要解决些什么问题？2.storm作为实时计算到底有何优势？storm简介官方介绍：Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!翻译：Apache Storm是一个免费的开源分布式实时计算系统。Storm可以轻松可靠地处理无限数据流，实时处理Hadoop为批处理所做的工作。storm很简单，可以与任何编程语言一起使用，并且使用起来很有趣！简单的说就是：storm是一个分布式实时计算系统。1.实时计算需要解决些什么问题？伴随着信息科技的日新月异的发展，信息呈现出爆发式的膨胀，人们获取信息的渠道也更加多元化，获取信息的方式也更加便捷，对信息的实效性也越来越高，举个电商系统一个搜索的简单例子，当卖家发布一条宝贝信息时，买家在搜索的时候需要能够马上呈现出来，同时买家购买后，能够需要统计该商品的卖出的数量以及该商品总营业额，利润等等。而且可以根据最近的购买的记录，可以给用户推荐同类型的产品。诸如此类的都需要以大数据为基础的，通过离线或者实时计算，获取相关的信息，从而获得商机。在Storm之前，进行实时处理是非常痛苦的事情: 需要维护一堆消息队列和消费者，他们构成了非常复杂的图结构。消费者进程从队列里取消息，处理完成后，去更新数据库，或者给其他队列发新消息。这样进行实时处理是非常痛苦的。我们主要的时间都花在关注往哪里发消息，从哪里接收消息，消息如何序列化，真正的业务逻辑只占了源代码的一小部分。一个应用程序的逻辑运行在很多worker上，但这些worker需要各自单独部署，还需要部署消息队列。最大问题是系统很脆弱，而且不是容错的：需要自己保证消息队列和worker进程工作正常。例如上面的例子，简单的统计数量，统计营业额，计算毛利润，根据购买记录，推荐相似商品等等，就是通过，开启多个工作线程，实时去扫表，或者是从消息对列中拿出数据，计算统计值，写入对应的表。这种定时任务，往往数据处理也不够及时，实效性比较差。2.实时计算系统要考虑哪些问题？低延迟 处理消息一定要及时，延迟高就不叫实时了。高性能 性能不高，那么就要浪费机器，这样浪费机器就是浪费资源分布式 系统数据和来源可能有多个，处理数据结果也有可能作为基础数据给其他系统。如果你的系统应用单机就能搞定，那么不需要考虑这么复杂了，实时计算系统就是为了解决这种单机系统无法解决的问题的。可扩展 伴随业务的的发展，我们的业务量，及计算量可能会越来越大，系统是要求可以扩展的，容错性 这个是分布式系统的通用问题了，一个节点挂了，不能影响到整个系统。3.storm的优势简单的编程模型。类似于MapReduce降低了批处理的复杂性，storm降低了进行实时处理的复杂性服务化，一个服务框架，支持热部署，即时上线或者下线app支持多种语言，你可以在storm之上使用各种编程语言，默认支持的有clojure,java,ruby,python容错性，storm会管理工作进程和节点故障水平扩展性，计算是在多个系统、进程、服务器之间进行的。可以水平扩展机器，进程，或者线程等。可靠的消息处理 storm 能够保证消息至少能够得到一次完整的消息处理。任务失败时，它会负责从消息源头重新处理。快速 系统的设计保证消息的快速处理，低版本的storm使用的zeroMQ作为内部消息系统。高版本中使用netty完全替代了ZeroMQ作为内部消息系统本地模式 storm 又一个本地模式，能够模拟集群，使我们的开发测试变得更加简单。storm的基本概念拓扑(Topologies)实时应用程序的逻辑被打包到Storm拓扑中。Storm拓扑类似于MapReduce作业。一个关键的区别是MapReduce作业最终完成，而拓扑结构永远运行（当然，直到你杀死它）。个拓扑是一个通过流分组(stream grouping)把Spout和Bolt连接到一起的拓扑结构。图的每条边代表一个Bolt订阅了其他Spout或者Bolt的输出流。一个拓扑就是一个复杂的多阶段的流计算。元组(Tuple)元组是Storm提供的一个轻量级的数据格式，可以用来包装你需要实际处理的数据。元组是一次消息传递的基本单元。一个元组是一个命名的值列表，其中的每个值都可以是任意类型的。元组是动态地进行类型转化的–字段的类型不需要事先声明。在Storm中编程时，就是在操作和转换由元组组成的流。通常，元组包含整数，字节，字符串，浮点数，布尔值和字节数组等类型。要想在元组中使用自定义类型，就需要实现自己的序列化方式。流(Streams)流是Storm中的核心抽象。一个流由无限的元组序列组成，这些元组会被分布式并行地创建和处理。通过流中元组包含的字段名称来定义这个流。每个流声明时都被赋予了一个ID。只有一个流的Spout和Bolt非常常见，所以OutputFieldsDeclarer提供了不需要指定ID来声明一个流的函数(Spout和Bolt都需要声明输出的流)。这种情况下，流的ID是默认的“default”。Spouts(喷嘴)Spout(喷嘴，这个名字很形象)是Storm中流的来源。通常Spout从外部数据源，如消息队列中读取元组数据并吐到拓扑里。Spout可以是可靠的(reliable)或者不可靠(unreliable)的。可靠的Spout能够在一个元组被Storm处理失败时重新进行处理，而非可靠的Spout只是吐数据到拓扑里，不关心处理成功还是失败了。Spout可以一次给多个流吐数据。此时需要通过OutputFieldsDeclarer的declareStream函数来声明多个流并在调用SpoutOutputCollector提供的emit方法时指定元组吐给哪个流。Spout中最主要的函数是nextTuple，Storm框架会不断调用它去做元组的轮询。如果没有新的元组过来，就直接返回，否则把新元组吐到拓扑里。nextTuple必须是非阻塞的，因为Storm在同一个线程里执行Spout的函数。Spout中另外两个主要的函数是ack和fail。当Storm检测到一个从Spout吐出的元组在拓扑中成功处理完时调用ack,没有成功处理完时调用fail。只有可靠型的Spout会调用ack和fail函数。Bolts在拓扑中所有的计算逻辑都是在Bolt中实现的。一个Bolt可以处理任意数量的输入流，产生任意数量新的输出流。Bolt可以做函数处理，过滤，流的合并，聚合，存储到数据库等操作。Bolt就是流水线上的一个处理单元，把数据的计算处理过程合理的拆分到多个Bolt、合理设置Bolt的task数量，能够提高Bolt的处理能力，提升流水线的并发度。Bolt可以给多个流吐出元组数据。此时需要使用OutputFieldsDeclarer的declareStream方法来声明多个流并在使用OutputColletor的emit方法时指定给哪个流吐数据。当你声明了一个Bolt的输入流，也就订阅了另外一个组件的某个特定的输出流。如果希望订阅另一个组件的所有流，需要单独挨个订阅。InputDeclarer有语法糖来订阅ID为默认值的流。例如declarer.shuffleGrouping(“redBolt”)订阅了redBolt组件上的默认流，跟declarer.shuffleGrouping(“redBolt”, DEFAULT_STREAM_ID)是相同的。在Bolt中最主要的函数是execute函数，它使用一个新的元组当作输入。Bolt使用OutputCollector对象来吐出新的元组。Bolts必须为处理的每个元组调用OutputCollector的ack方法以便于Storm知道元组什么时候被各个Bolt处理完了（最终就可以确认Spout吐出的某个元组处理完了）。通常处理一个输入的元组时，会基于这个元组吐出零个或者多个元组，然后确认(ack)输入的元组处理完了，Storm提供了IBasicBolt接口来自动完成确认。必须注意OutputCollector不是线程安全的，所以所有的吐数据(emit)、确认(ack)、通知失败(fail)必须发生在同一个线程里任务(Tasks)每个Spout和Bolt会以多个任务(Task)的形式在集群上运行。每个任务对应一个执行线程，流分组定义了如何从一组任务(同一个Bolt)发送元组到另外一组任务(另外一个Bolt)上。可以在调用TopologyBuilder的setSpout和setBolt函数时设置每个Spout和Bolt的并发数。组件(Component)组件(component)是对Bolt和Spout的统称流分组(Stream groupings)定义拓扑的时候，一部分工作是指定每个Bolt应该消费哪些流。流分组定义了一个流在一个消费它的Bolt内的多个任务(task)之间如何分组。流分组跟计算机网络中的路由功能是类似的，决定了每个元组在拓扑中的处理路线。在Storm中有七个内置的流分组策略，你也可以通过实现CustomStreamGrouping接口来自定义一个流分组策略:洗牌分组(Shuffle grouping): 随机分配元组到Bolt的某个任务上，这样保证同一个Bolt的每个任务都能够得到相同数量的元组。字段分组(Fields grouping): 按照指定的分组字段来进行流的分组。例如，流是用字段“user-id”来分组的，那有着相同“user-id”的元组就会分到同一个任务里，但是有不同“user-id”的元组就会分到不同的任务里。这是一种非常重要的分组方式，通过这种流分组方式，我们就可以做到让Storm产出的消息在这个”user-id”级别是严格有序的，这对一些对时序敏感的应用(例如，计费系统)是非常重要的。Partial Key grouping: 跟字段分组一样，流也是用指定的分组字段进行分组的，但是在多个下游Bolt之间是有负载均衡的，这样当输入数据有倾斜时可以更好的利用资源。这篇论文很好的解释了这是如何工作的，有哪些优势。All grouping: 流会复制给Bolt的所有任务。小心使用这种分组方式。在拓扑中，如果希望某类元祖发送到所有的下游消费者，就可以使用这种All grouping的流分组策略。Global grouping: 整个流会分配给Bolt的一个任务。具体一点，会分配给有最小ID的任务。不分组(None grouping): 说明不关心流是如何分组的。目前，None grouping等价于洗牌分组。Direct grouping：一种特殊的分组。对于这样分组的流，元组的生产者决定消费者的哪个任务会接收处理这个元组。只能在声明做直连的流(direct streams)上声明Direct groupings分组方式。只能通过使用emitDirect系列函数来吐元组给直连流。一个Bolt可以通过提供的TopologyContext来获得消费者的任务ID，也可以通过OutputCollector对象的emit函数(会返回元组被发送到的任务的ID)来跟踪消费者的任务ID。在ack的实现中，Spout有两个直连输入流，ack和ackFail，使用了这种直连分组的方式。Local or shuffle grouping：如果目标Bolt在同一个worker进程里有一个或多个任务，元组就会通过洗牌的方式分配到这些同一个进程内的任务里。否则，就跟普通的洗牌分组一样。这种方式的好处是可以提高拓扑的处理效率，因为worker内部通信就是进程内部通信了，相比拓扑间的进程间通信要高效的多。worker进程间通信是通过使用Netty来进行网络通信的。可靠性(Reliability)Storm保证了拓扑中Spout产生的每个元组都会被处理。Storm是通过跟踪每个Spout所产生的所有元组构成的树形结构并得知这棵树何时被完整地处理来达到可靠性。每个拓扑对这些树形结构都有一个关联的“消息超时”。如果在这个超时时间里Storm检测到Spout产生的一个元组没有被成功处理完，那Sput的这个元组就处理失败了，后续会重新处理一遍。为了发挥Storm的可靠性，需要你在创建一个元组树中的一条边时告诉Storm，也需要在处理完每个元组之后告诉Storm。这些都是通过Bolt吐元组数据用的OutputCollector对象来完成的。标记是在emit函数里完成，完成一个元组后需要使用ack函数来告诉Storm。Workers(工作进程)拓扑以一个或多个Worker进程的方式运行。每个Worker进程是一个物理的Java虚拟机，执行拓扑的一部分任务。例如，如果拓扑的并发设置成了300，分配了50个Worker，那么每个Worker执行6个任务(作为Worker内部的线程）。Storm会尽量把所有的任务均分到所有的Worker上。storm的工作流程Storm实现了一个数据流(data flow)的模型，在这个模型中数据持续不断地流经一个由很多转换实体构成的网络。一个数据流的抽象叫做流(stream)，流是无限的元组(Tuple)序列。元组就像一个可以表示标准数据类型（例如int，float和byte数组）和用户自定义类型（需要额外序列化代码的）的数据结构。每个流由一个唯一的ID来标示的，这个ID可以用来构建拓扑中各个组件的数据源。如下图所示，其中的水龙头代表了数据流的来源，一旦水龙头打开，数据就会源源不断地流经Bolt而被处理。图中有三个流，用不同的颜色来表示，每个数据流中流动的是元组(Tuple)，它承载了具体的数据。元组通过流经不同的转换实体而被处理。Storm对数据输入的来源和输出数据的去向没有做任何限制。像Hadoop，是需要把数据放到自己的文件系统HDFS里的。在Storm里，可以使用任意来源的数据输入和任意的数据输出，只要你实现对应的代码来获取/写入这些数据就可以。典型场景下，输入/输出数据来是基于类似Kafka或者ActiveMQ这样的消息队列，但是数据库，文件系统或者web服务也都是可以的。如图：storm 测试用例并运行往往我在学习这些开源框架的时候，查看官方文档和源码中的例子是入门上手比较快的一种方式。这里我也是从官方文档和github上的代码入手的1.clone 下来1.2.2的源码1git clone --branch v1.2.2 https://github.com/apache/storm.git2.进入examples目录下有很多例子，如storm-starter这个项目。同时在github上进入这个例子的目录下有README.md文件，介绍如何运行我们的测试例子。我们可以先感官体验一下storm运行拓扑是怎样的。详情请看storm-starterstep 1：用idea 打开storm源码 然后用maven打包或者进入storm-starter项目里面执行1mvn package会在target目录里面生成一个start-storm-{version}.jar包上传到storm的服务器。step 2: 提交运行实例拓扑，有本地和集群两种模式。是不是本地模式🉐️看代码如何实现的。12storm jar stormlib/storm-starter-1.2.2.jar org.apache.storm.starter.ExclamationTopology ##本地模式storm jar stormlib/storm-starter-1.2.2.jar org.apache.storm.starter.ExclamationTopology ExclamationTopology ## ExclamationTopology为Topology的名字step 3: org.apache.storm.starter.ExclamationTopology 这个拓扑类如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192/** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.storm.starter;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.testing.TestWordSpout;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.apache.storm.utils.Utils;import java.util.Map;/** * This is a basic example of a Storm topology. */public class ExclamationTopology &#123; public static class ExclamationBolt extends BaseRichBolt &#123; OutputCollector _collector; @Override public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123; _collector = collector; &#125; @Override public void execute(Tuple tuple) &#123; _collector.emit(tuple, new Values(tuple.getString(0) + \"!!!\")); _collector.ack(tuple); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"word\")); &#125; &#125; public static void main(String[] args) throws Exception &#123; //构建拓扑类 TopologyBuilder builder = new TopologyBuilder(); //设置数据获取来源spout builder.setSpout(\"word\", new TestWordSpout(), 10); //设置数据处理bolt类按照word分组 builder.setBolt(\"exclaim1\", new ExclamationBolt(), 3).shuffleGrouping(\"word\"); //设置数据处理bolt类按照exclaim1分组 builder.setBolt(\"exclaim2\", new ExclamationBolt(), 2).shuffleGrouping(\"exclaim1\"); //设置配置参数，打开debug模式 Config conf = new Config(); conf.setDebug(true); // 根据传入参数创建对应拓扑，如果参数大于0，就提交到storm集群，集群模式运行 if (args != null &amp;&amp; args.length &gt; 0) &#123; conf.setNumWorkers(3); //通过nimbus提交拓扑到suppervisisor工作节点运行 StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology()); &#125; else &#123; //否则就是local本地模式运行，本地模式运行所有的日志都会打印到本地，可以用来调试 LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"test\", conf, builder.createTopology()); Utils.sleep(10000); //运行10秒后杀掉拓扑 cluster.killTopology(\"test\"); //同时释放资源，关掉storm cluster.shutdown(); &#125; &#125;&#125;org.apache.storm.testing.TestWordSpout;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.storm.testing;import org.apache.storm.Config;import org.apache.storm.topology.OutputFieldsDeclarer;import java.util.Map;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import org.apache.storm.utils.Utils;import java.util.HashMap;import java.util.Random;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class TestWordSpout extends BaseRichSpout &#123; public static Logger LOG = LoggerFactory.getLogger(TestWordSpout.class); boolean _isDistributed; SpoutOutputCollector _collector; public TestWordSpout() &#123; this(true); &#125; public TestWordSpout(boolean isDistributed) &#123; _isDistributed = isDistributed; &#125; public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; _collector = collector; &#125; public void close() &#123; &#125; public void nextTuple() &#123; Utils.sleep(100); final String[] words = new String[] &#123;\"nathan\", \"mike\", \"jackson\", \"golda\", \"bertels\"&#125;; final Random rand = new Random(); final String word = words[rand.nextInt(words.length)]; //随机从这些字符串中获取数据 _collector.emit(new Values(word)); &#125; public void ack(Object msgId) &#123; &#125; public void fail(Object msgId) &#123; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"word\")); &#125; @Override public Map&lt;String, Object&gt; getComponentConfiguration() &#123; if(!_isDistributed) &#123; Map&lt;String, Object&gt; ret = new HashMap&lt;String, Object&gt;(); ret.put(Config.TOPOLOGY_MAX_TASK_PARALLELISM, 1); return ret; &#125; else &#123; return null; &#125; &#125; &#125;storm API 简单介绍1.拓扑构建TopologyBuilder公开了Java API，用于指定要执行的Storm拓扑。拓扑结构最终是Thrift结构，但由于Thrift API非常冗长，TopologyBuilder极大地简化了创建拓扑的过程。用于创建和提交拓扑的模板类似于：12345678910111213TopologyBuilder builder = new TopologyBuilder();builder.setSpout(\"1\", new TestWordSpout(true), 5);builder.setSpout(\"2\", new TestWordSpout(true), 3);builder.setBolt(\"3\", new TestWordCounter(), 3) .fieldsGrouping(\"1\", new Fields(\"word\")) .fieldsGrouping(\"2\", new Fields(\"word\"));builder.setBolt(\"4\", new TestGlobalCount()) .globalGrouping(\"1\");Map conf = new HashMap();conf.put(Config.TOPOLOGY_WORKERS, 4);StormSubmitter.submitTopology(\"mytopology\", conf, builder.createTopology());在本地模式（正在处理）中运行完全相同的拓扑，并将其配置为记录所有发出的元组，如下所示。请注意，在关闭本地群集之前，它允许拓扑运行10秒。123456789101112131415161718TopologyBuilder builder = new TopologyBuilder();builder.setSpout(\"1\", new TestWordSpout(true), 5);builder.setSpout(\"2\", new TestWordSpout(true), 3);builder.setBolt(\"3\", new TestWordCounter(), 3) .fieldsGrouping(\"1\", new Fields(\"word\")) .fieldsGrouping(\"2\", new Fields(\"word\"));builder.setBolt(\"4\", new TestGlobalCount()) .globalGrouping(\"1\");Map conf = new HashMap();conf.put(Config.TOPOLOGY_WORKERS, 4);conf.put(Config.TOPOLOGY_DEBUG, true);LocalCluster cluster = new LocalCluster();cluster.submitTopology(\"mytopology\", conf, builder.createTopology());Utils.sleep(10000);cluster.shutdown();模式TopologyBuilder是使用setSpout和setBolt方法将组件ID映射到组件。这些方法返回的对象随后用于声明该组件的输入。详情可以查看TopologyBuilder2.创建spout相关ISpout接口：123456789101112public interface ISpout extends Serializable &#123; //初始化方法 void open(Map conf, TopologyContext context, SpoutOutputCollector collector); //关闭 void close(); //遍历元组的方法，会一直执行这个方法 void nextTuple(); //成功时调用的确认方法 void ack(Object msgId); //失败时调用的失败方法 void fail(Object msgId);&#125;storm已经给我们提供的很多的spout接口和实现类，我们只需要实现或者对应的实现类就能和其他技术集成在一起。基本的spout，我们可以实现IRichSpout或者继承BasicRichSpout完成spout的创建。3.创建Bolt类storm已经给我们提供的很多的Bolt接口和实现类，我们只需要实现或者对应的实现类就能和其他技术集成在一起。基本的spout，我们可以实现IRichBolt或者继承BasicRichBolt如1234567891011121314151617181920212223/*** BaseRichBolt 是一个不需要实现的ACK确认方法和fail（）失败方法* */public class ExampleBolt extends BaseRichBolt &#123; OutputCollector _collector; @Override public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123; _collector = collector; &#125; @Override public void execute(Tuple tuple) &#123; _collector.emit(tuple, new Values(tuple.getString(0) + \"!!!\")); _collector.ack(tuple); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"word\")); &#125; &#125;Storm使用入门起来是非常简单的。下面我将简单的自己实现一个拓扑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146package com.sonly.storm.demo1;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.StormSubmitter;import org.apache.storm.generated.AlreadyAliveException;import org.apache.storm.generated.AuthorizationException;import org.apache.storm.generated.InvalidTopologyException;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)HelloToplogy&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:55&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class HelloToplogy &#123; public static final Logger LOGGER = LoggerFactory.getLogger(HelloToplogy.class); //Topology Name //component prefix //workers //spout executor (parallelism_hint) //spout task size //bolt executor (parallelism_hint) //bolt task size public static void main(String[] args) throws InterruptedException &#123; TopologyBuilder builder = new TopologyBuilder(); Config conf = new Config(); conf.setDebug(true); if (args==null || args.length &lt; 7) &#123; conf.setNumWorkers(3); builder.setSpout(\"spout\", new HellowordSpout(), 4).setNumTasks(4); builder.setBolt(\"split-bolt\", new SplitBolt(), 4).shuffleGrouping(\"spout\").setNumTasks(8); builder.setBolt(\"count-bolt\", new HellowordBolt(), 8).fieldsGrouping(\"split-bolt\", new Fields(\"word\")).setNumTasks(8); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"word-count\", conf, builder.createTopology()); Thread.sleep(10000); cluster.killTopology(\"word-count\"); cluster.shutdown(); &#125; else &#123; Options options = Options.builder(args); conf.setNumWorkers(options.getWorkers()); builder.setSpout(options.getPrefix()+\"-spout\", new HellowordSpout(), options.getSpoutParallelismHint()).setNumTasks(options.getSpoutTaskSize()); builder.setBolt(options.getPrefix()+\"-split-bolt\", new SplitBolt(), options.getBoltParallelismHint()).shuffleGrouping(options.getPrefix()+\"-spout\").setNumTasks(options.getBoltTaskSize()); builder.setBolt(options.getPrefix()+\"-count-bolt\", new HellowordBolt(), options.getBoltParallelismHint()).fieldsGrouping(options.getPrefix()+\"-split-bolt\", new Fields(\"word\")).setNumTasks(options.getBoltTaskSize()); try &#123; StormSubmitter.submitTopologyWithProgressBar(options.getTopologyName(), conf, builder.createTopology()); LOGGER.warn(\"===========================================================\"); LOGGER.warn(\"The Topology &#123;&#125; is Submited \",options.getTopologyName()); LOGGER.warn(\"===========================================================\"); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static class Options&#123; private String topologyName; private String prefix; private Integer workers; private Integer spoutParallelismHint; private Integer spoutTaskSize; private Integer boltParallelismHint; private Integer boltTaskSize; public Options(String topologyName, String prefix, Integer workers, Integer spoutParallelismHint, Integer spoutTaskSize, Integer boltParallelismHint, Integer boltTaskSize) &#123; this.topologyName = topologyName; this.prefix = prefix; this.workers = workers; this.spoutParallelismHint = spoutParallelismHint; this.spoutTaskSize = spoutTaskSize; this.boltParallelismHint = boltParallelismHint; this.boltTaskSize = boltTaskSize; &#125; public static Options builder(String[] args)&#123; return new Options(args[0],args[1],Integer.parseInt(args[2]) ,Integer.parseInt(args[3]),Integer.parseInt(args[4]),Integer.parseInt(args[5]),Integer.parseInt(args[6]) ); &#125; public String getTopologyName() &#123; return topologyName; &#125; public void setTopologyName(String topologyName) &#123; this.topologyName = topologyName; &#125; public String getPrefix() &#123; return prefix; &#125; public void setPrefix(String prefix) &#123; this.prefix = prefix; &#125; public Integer getWorkers() &#123; return workers; &#125; public void setWorkers(Integer workers) &#123; this.workers = workers; &#125; public Integer getSpoutParallelismHint() &#123; return spoutParallelismHint; &#125; public void setSpoutParallelismHint(Integer spoutParallelismHint) &#123; this.spoutParallelismHint = spoutParallelismHint; &#125; public Integer getSpoutTaskSize() &#123; return spoutTaskSize; &#125; public void setSpoutTaskSize(Integer spoutTaskSize) &#123; this.spoutTaskSize = spoutTaskSize; &#125; public Integer getBoltParallelismHint() &#123; return boltParallelismHint; &#125; public void setBoltParallelismHint(Integer boltParallelismHint) &#123; this.boltParallelismHint = boltParallelismHint; &#125; public Integer getBoltTaskSize() &#123; return boltTaskSize; &#125; public void setBoltTaskSize(Integer boltTaskSize) &#123; this.boltTaskSize = boltTaskSize; &#125; &#125;&#125;spout 类：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.sonly.storm.demo1;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Currency;import java.util.Map;import java.util.Random;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;HellowordSpout&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 20:27&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class HellowordSpout extends BaseRichSpout &#123; public static final Logger LOGGER = LoggerFactory.getLogger(HellowordSpout.class); //拓扑上下文 private TopologyContext context; private SpoutOutputCollector collector; private Map config; private Random random; public void open(Map conf, TopologyContext topologyContext, SpoutOutputCollector collector) &#123; this.config = conf; this.context = topologyContext; this.collector = collector; this.random = new Random(); LOGGER.warn(\"HellowordSpout-&gt;open:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void nextTuple() &#123; String[] sentences = new String[]&#123;\"hello world !\", \"hello Storm !\", \"hello apache flink !\", \"hello apache kafka stream !\", \"hello apache spark !\"&#125;; final String sentence = sentences[random.nextInt(sentences.length)]; collector.emit(new Values(sentence)); LOGGER.warn(\"HellowordSpout-&gt;nextTuple:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;,Values:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId(),sentence); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"sentence\")); &#125; @Override public void close() &#123; LOGGER.warn(\"HellowordSpout-&gt;close:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); super.close(); &#125;&#125;实现两个bolt一个用来统计单词出现个数，一个用来拆分语句。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.sonly.storm.demo1;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.Map;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:19&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class HellowordBolt extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(HellowordBolt.class); private TopologyContext context; private Map conf; private OutputCollector collector; private Map&lt;String,Integer&gt; counts = new HashMap(16); public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.conf=map; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"HellowordBolt-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; LOGGER.warn(\"HellowordBolt-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); String word = tuple.getString(0); Integer count = counts.get(word); if (count == null) count = 0; count++; counts.put(word, count); collector.emit(new Values(word, count)); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"word\", \"count\")); &#125;&#125;12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.sonly.storm.demo1;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;/** * &lt;b&gt;package:com.sonly.storm.demo1&lt;/b&gt; * &lt;b&gt;project(项目):stormstudy&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-09 21:29&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class SplitBolt extends BaseRichBolt &#123; public static final Logger LOGGER = LoggerFactory.getLogger(SplitBolt.class); private TopologyContext context; private Map conf; private OutputCollector collector; public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) &#123; this.conf=map; this.context = topologyContext; this.collector = outputCollector; LOGGER.warn(\"SplitBolt-&gt;prepare:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void execute(Tuple tuple) &#123; String words = tuple.getStringByField(\"sentence\"); String[] contents = words.split(\" +\"); for (String content : contents) &#123; collector.emit(new Values(content)); collector.ack(tuple); &#125; LOGGER.warn(\"SplitBolt-&gt;execute:hashcode:&#123;&#125;-&gt;ThreadId:&#123;&#125;,TaskId:&#123;&#125;\",this.hashCode(),Thread.currentThread().getId(),context.getThisTaskId()); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"word\")); &#125;&#125;local模式启动运行在pom文件中添加打包插件12345678910111213&lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.sonly.storm.demo1.HelloToplogy&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt;&lt;/plugin&gt;同时修改dependency 的scope为provide1&lt;scope&gt;provide&lt;/scope&gt;原因是服务器上storm相关包都已经存在了，防止重复打包导致冲突。1234567//Topology Name//component prefix//workers//spout executor (parallelism_hint)//spout task size//bolt executor (parallelism_hint)//bolt task size打包上传后，storm jar jarName arg0 arg1 arg2 args3 …后面跟参数运行即可。","categories":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/categories/storm/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/tags/storm/"}]},{"title":"storm 安装使用","slug":"storm-install","date":"2019-05-02T13:37:40.000Z","updated":"2019-05-08T09:39:18.278Z","comments":true,"path":"2019/05/02/storm-install/","link":"","permalink":"http://www.liuyong520.cn/2019/05/02/storm-install/","excerpt":"","text":"环境准备zookeeper集群环境storm是依赖于zookeeper注册中心的一款分布式消息对列，所以需要有zookeeper单机或者集群环境。准备三台服务器：123172.16.18.198 k8s-n1172.16.18.199 k8s-n2172.16.18.200 k8s-n3下载storm安装包http://storm.apache.org/downloads.html 中下载，目前最新版本的strom已经到1.2.2,我这里之前下载的是1.1.3版本的。安装storm集群上传压缩包到三台服务器解压缩到/opt/目录下12tar -zxf apache-storm-1.1.3.tar.gz -C /optln -sf apache-storm-1.1.3/ storm修改 conf目录下的storm.yml文件Storm包含一个conf/storm.yaml配置Storm守护进程的文件。这个文件里面配置的值会覆盖掉default.yml里面的值，同时里面有一些配置是必须填的注意：yml文件的前面的空格必须有，不然就会出问题，yml配置文件有严格的格式1)storm.zookeeper.servers：这是Storm集群的Zookeeper集群中的主机列表。它应该看起来像：1234storm.zookeeper.servers: - &quot;k8s-n1&quot; - &quot;k8s-n2 - &quot;k8s-n3&quot;2)如果zookeeper的默认端口不是2181的话还需要配置storm.zookeeper.port,如果是2181，此选项可以不用配置1storm.zookeeper.port: 21813）storm.local.dir：Nimbus和Supervisor守护进程需要本地磁盘上的一个目录来存储少量状态（如jar，confs和类似的东西）。您应该在每台计算机上创建该目录，为其提供适当的权限，然后使用此配置填写目录位置。例如：1storm.local.dir: &quot;/data/storm&quot;4）nimbus.seeds：工作节点需要知道哪些机器是主机的候选者才能下载拓扑罐和confs。例如：1nimbus.seeds: [&quot;k8s-n1&quot;,&quot;k8s-n2&quot;,&quot;k8s-n3&quot;]4）supervisor.slots.ports：对于每个工作者计算机，您可以使用此配置配置在该计算机上运行的工作程序数。每个工作人员使用单个端口接收消息，此设置定义哪些端口可以使用。如果您在此处定义了五个端口，那么Storm将分配最多五个工作人员在此计算机上运行。如果您定义了三个端口，Storm最多只能运行三个端口。默认情况下，此设置配置为在端口6700,6701,6702和6703上运行4个工作程序。例如：12345supervisor.slots.ports: - 6700 - 6701 - 6702 - 67035)开启监督机制监督健康状况Storm提供了一种机制，管理员可以通过该机制定期管理员定期运行管理员提供的脚本，以确定节点是否健康。管理员可以让主管通过在storm.health.check.dir中的脚本中执行他们选择的任何检查来确定节点是否处于健康状态。如果脚本检测到节点处于不健康状态，则必须从标准输出打印一行，以字符串ERROR开头。主管将定期运行运行状况检查目录中的脚本并检查输出。如果脚本的输出包含字符串ERROR，如上所述，主管将关闭所有工作人员并退出。如果主管在监督下运行，则可以调用“/ bin / storm node-health-check”来确定是否应该启动主管或节点是否运行状况不佳。运行状况检查目录位置可以配置为：1storm.health.check.dir: &quot;healthchecks&quot;脚本必须具有执行权限。允许任何给定的运行状况检查脚本在由于超时而标记为失败之前运行的时间可以配置为：1storm.health.check.timeout.ms: 5000启动Nimbus：在主机监督下运行命令“bin / storm nimbus”。主管：在每台工作机器的监督下运行命令“bin / storm supervisor”。管理程序守护程序负责启动和停止该计算机上的工作进程。UI：通过在监督下运行命令“bin / storm ui”，运行Storm UI（您可以从浏览器访问的站点，该站点提供对群集和拓扑的诊断）。可以通过将Web浏览器导航到http：// {ui host}：8080来访问UI。如您所见，运行守护进程非常简单。守护程序将在您解压缩Storm版本的任何位置登录到logs /目录。后台启动1nohup storm ui &gt;/dev/null 2&gt;&amp;1 &amp;","categories":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/categories/storm/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://www.liuyong520.cn/tags/storm/"}]},{"title":"kafka API的使用","slug":"kafka-API","date":"2019-05-02T13:37:40.000Z","updated":"2019-05-03T13:16:45.720Z","comments":true,"path":"2019/05/02/kafka-API/","link":"","permalink":"http://www.liuyong520.cn/2019/05/02/kafka-API/","excerpt":"","text":"kafka APIkafka Consumer提供两套Java API：高级Consumer API、和低级Consumer API。高级Consumer API 优点：高级API写起来简单，易用。不需要自行去管理offset，API已经封装好了offset这块的东西，会通过zookeeper自行管理不需要管理分区，副本等情况，系统自动管理消费者断线后会自动根据上次记录在zookeeper中的offset接着消费消息。高级Consumer API 缺点：不能自行控制offset。不能自行管理分区，副本，zk等相关信息。低级API 优点：能够让开发者自己维护offset.想从哪里消费就从哪里消费自行控制连接分区，对分区自定义负载均衡对zookeeper的依赖性降低（如 offset 不一定要用zk来存储，可以存在缓存里或者内存中）缺点：过于复杂，需要自行控制offset，连接哪个分区，找分区leader等。简单入门使用引入maven依赖12345dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt;Producer简单使用1234567891011121314151617181920212223242526272829303132package com.sonly.kafka;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;/** * &lt;b&gt;package:com.sonly.kafka&lt;/b&gt; * &lt;b&gt;project(项目):kafkaAPIdemo&lt;/b&gt; * &lt;b&gt;class(类)demo&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-03 12:17&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class demo &#123; public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"k8s-n1:9092\"); properties.put(ProducerConfig.ACKS_CONFIG,\"1\"); properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); for (int i = 0; i &lt; 100; i++) producer.send(new ProducerRecord&lt;String, String&gt;(\"mytest\", Integer.toString(i), Integer.toString(i))); producer.close(); &#125;&#125;带回调函数的生产者123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.sonly.kafka;import org.apache.kafka.clients.producer.*;import java.util.Properties;/** * &lt;b&gt;package:com.sonly.kafka&lt;/b&gt; * &lt;b&gt;project(项目):kafkaAPIdemo&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-03 12:58&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class demo1 &#123; public static void main(String[] args) &#123; Properties properties = new Properties(); //设置kafka集群 properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"k8s-n1:9092\"); //设置brokeACK应答机制 properties.put(ProducerConfig.ACKS_CONFIG,\"1\"); //设置key序列化 properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); //设置value序列化 properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); //设置批量大小 properties.put(ProducerConfig.BATCH_SIZE_CONFIG,\"6238\"); //设置提交延时 properties.put(ProducerConfig.LINGER_MS_CONFIG,\"1\"); //设置producer缓存 properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG,Long.MAX_VALUE); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); for ( int i = 0; i &lt; 12; i++) &#123; final int finalI = i; producer.send(new ProducerRecord&lt;String, String&gt;(\"mytest\", Integer.toString(i), Integer.toString(i)), new Callback() &#123; public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if(exception==null)&#123; System.out.println(\"发送成功: \" + finalI +\",\"+metadata.partition()+\",\"+ metadata.offset()); &#125; &#125; &#125;); &#125; producer.close(); &#125;&#125;结果：123456789101112发送成功: 0,0,170发送成功: 2,0,171发送成功: 11,0,172发送成功: 4,1,101发送成功: 5,2,116发送成功: 6,2,117发送成功: 10,2,118发送成功: 1,3,175发送成功: 3,3,176发送成功: 7,3,177发送成功: 8,3,178发送成功: 9,3,179数据不均等的分配到0-3 号分区上自定义分区发送12345678910111213141516171819202122232425262728package com.sonly.kafka;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import java.util.Map;/** * &lt;b&gt;package:com.sonly.kafka&lt;/b&gt; * &lt;b&gt;project(项目):kafkaAPIdemo&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-03 13:43&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class CustomProducer implements Partitioner &#123; public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; return 0; &#125; public void close() &#123; &#125; public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125;设置分区12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.sonly.kafka;import org.apache.kafka.clients.producer.*;import java.util.Properties;/** * &lt;b&gt;package:com.sonly.kafka&lt;/b&gt; * &lt;b&gt;project(项目):kafkaAPIdemo&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-03 13:46&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class demo2 &#123; public static void main(String[] args) &#123; Properties properties = new Properties(); //设置kafka集群 properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,\"k8s-n1:9092\"); //设置brokeACK应答机制 properties.put(ProducerConfig.ACKS_CONFIG,\"1\"); //设置key序列化 properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); //设置value序列化 properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringSerializer\"); //设置批量大小 properties.put(ProducerConfig.BATCH_SIZE_CONFIG,\"6238\"); //设置提交延时 properties.put(ProducerConfig.LINGER_MS_CONFIG,\"1\"); //设置producer缓存 properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG,Long.MAX_VALUE); //设置partition properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,\"com.sonly.kafka.CustomProducer\"); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); for ( int i = 0; i &lt; 12; i++) &#123; final int finalI = i; producer.send(new ProducerRecord&lt;String, String&gt;(\"mytest\", Integer.toString(i), Integer.toString(i)), new Callback() &#123; public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if(exception==null)&#123; System.out.println(\"发送成功: \" + finalI +\",\"+metadata.partition()+\",\"+ metadata.offset()); &#125; &#125; &#125;); &#125; producer.close(); &#125;&#125;消费者高级API：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.sonly.kafka.consumer;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.clients.producer.ProducerConfig;import java.util.Arrays;import java.util.Properties;/** * &lt;b&gt;package:com.sonly.kafka.consumer&lt;/b&gt; * &lt;b&gt;project(项目):kafkaAPIdemo&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-03 13:59&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class ConsumerDemo &#123; public static void main(String[] args) &#123; Properties properties = new Properties(); //设置kafka集群 properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\"k8s-n1:9092\"); //设置brokeACK应答机制 properties.put(ConsumerConfig.GROUP_ID_CONFIG,\"teste3432\"); //设置key反序列化 properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringDeserializer\"); //设置value反序列化 properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,\"org.apache.kafka.common.serialization.StringDeserializer\"); //设置拿取大小 properties.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG,100*1024*1024); //设置自动提交offset properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,true); //设置自动提交延时 properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,1000); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(properties); consumer.subscribe(Arrays.asList(\"mytest\",\"test\")); while (true)&#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(10); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record.topic()+\"--\"+record.partition()+\"--\"+record.value()); &#125; &#125; &#125;&#125;低级API：1.消费者使用低级API的主要步骤步骤主要工作1根据指定分区从topic元数据中找到leader2获取分区最新的消费进度3从主副本中拉取分区消息4识别主副本的变化，重试2.方法描述：方法描述findLeader()客户端向种子阶段发送主题元数据，将副本加入备用节点getLastOffset()消费者客户端发送偏移量请求，获取分区最近的偏移量run()消费者低级API拉取消息的方法findNewLeader()当分区主副本节点发生故障时，客户端将要找出新的主副本修改pom12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package com.sonly.kafka.consumer;import kafka.api.FetchRequest;import kafka.api.FetchRequestBuilder;import kafka.api.KAFKA_0_8_1$;import kafka.cluster.BrokerEndPoint;import kafka.javaapi.*;import kafka.javaapi.consumer.SimpleConsumer;import kafka.javaapi.message.ByteBufferMessageSet;import kafka.message.MessageAndOffset;import org.apache.kafka.clients.consumer.Consumer;import java.nio.ByteBuffer;import java.util.*;import java.util.concurrent.TimeUnit;/** * &lt;b&gt;package:com.sonly.kafka.consumer&lt;/b&gt; * &lt;b&gt;project(项目):kafkaAPIdemo&lt;/b&gt; * &lt;b&gt;class(类)$&#123;CLASS_NAME&#125;&lt;/b&gt; * &lt;b&gt;creat date(创建时间):2019-05-03 15:21&lt;/b&gt; * &lt;b&gt;author(作者):&lt;/b&gt;xxydliuyss&lt;/br&gt; * &lt;b&gt;note(备注)):&lt;/b&gt; * If you want to change the file header,please modify zhe File and Code Templates. */public class LowerConsumer &#123; //保存offset private long offset; //保存分区副本 private Map&lt;Integer,List&lt;BrokerEndPoint&gt;&gt; partitionsMap = new HashMap&lt;Integer, List&lt;BrokerEndPoint&gt;&gt;(1024); public static void main(String[] args) throws InterruptedException &#123; List&lt;String&gt; brokers = Arrays.asList(\"k8s-n1\", \"k8s-n2\",\"k8s-n3\"); int port = 9092; int partition = 1; long offset=2; LowerConsumer lowerConsumer = new LowerConsumer(); while(true)&#123;// offset = lowerConsumer.getOffset(); lowerConsumer.getData(brokers,port,\"mytest\",partition,offset); TimeUnit.SECONDS.sleep(1); &#125; &#125; public long getOffset() &#123; return offset; &#125; private BrokerEndPoint findLeader(Collection&lt;String&gt; brokers,int port,String topic,int partition)&#123; for (String broker : brokers) &#123; //创建消费者对象操作每一台服务器 SimpleConsumer getLeader = new SimpleConsumer(broker, port, 10000, 1024 * 24, \"getLeader\"); //构造元数据请求 TopicMetadataRequest topicMetadataRequest = new TopicMetadataRequest(Collections.singletonList(topic)); //发送元数据请求 TopicMetadataResponse response = getLeader.send(topicMetadataRequest); //解析元数据 List&lt;TopicMetadata&gt; topicMetadatas = response.topicsMetadata(); //遍历数据 for (TopicMetadata topicMetadata : topicMetadatas) &#123; //获取分区元数据信息 List&lt;PartitionMetadata&gt; partitionMetadatas = topicMetadata.partitionsMetadata(); //遍历分区元数据 for (PartitionMetadata partitionMetadata : partitionMetadatas) &#123; if(partition == partitionMetadata.partitionId())&#123; //保存，分区对应的副本，如果需要主副本挂掉重新获取leader只需要遍历这个缓存即可 List&lt;BrokerEndPoint&gt; isr = partitionMetadata.isr(); this.partitionsMap.put(partition,isr); return partitionMetadata.leader(); &#125; &#125; &#125; &#125; return null; &#125; private void getData(Collection&lt;String&gt; brokers,int port,String topic,int partition,long offset)&#123; //获取leader BrokerEndPoint leader = findLeader(brokers, port, topic, partition); if(leader==null) return; String host = leader.host(); //获取数据的消费者对象 SimpleConsumer getData = new SimpleConsumer(host, port, 10000, 1024 * 10, \"getData\"); //构造获取数据request 这里一次可以添加多个topic addFecth 添加即可 FetchRequest fetchRequestBuilder = new FetchRequestBuilder().addFetch(topic, partition, offset, 1024 * 10).build(); //发送获取数据请求 FetchResponse fetchResponse = getData.fetch(fetchRequestBuilder); //解析元数据返回，这是message的一个set集合 ByteBufferMessageSet messageAndOffsets = fetchResponse.messageSet(topic, partition); //遍历消息集合 for (MessageAndOffset messageAndOffset : messageAndOffsets) &#123; long offset1 = messageAndOffset.offset(); this.setOffset(offset); ByteBuffer payload = messageAndOffset.message().payload(); byte[] buffer = new byte[payload.limit()]; payload.get(buffer); String message = new String(buffer); System.out.println(\"offset:\"+ offset1 +\"--message:\"+ message); &#125; &#125; private void setOffset(long offset) &#123; this.offset = offset; &#125;&#125;这个低级API在最新的kafka版本中已经不再提供了。","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://www.liuyong520.cn/categories/消息队列/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.liuyong520.cn/tags/kafka/"}]},{"title":"Linux下zookeeper集群搭建","slug":"zookeeper-install","date":"2019-04-29T02:56:51.000Z","updated":"2019-04-29T10:04:56.654Z","comments":true,"path":"2019/04/29/zookeeper-install/","link":"","permalink":"http://www.liuyong520.cn/2019/04/29/zookeeper-install/","excerpt":"","text":"部署前准备下载zookeeper的安装包http://zookeeper.apache.org/releases.html 我下载的版本是zookeeper-3.4.10。准备三台服务器ip地址为：123172.16.18.198172.16.18.199172.16.18.200检查jdk版本，安装jdk环境，jdk需要1.7以上。安装zookeeper三台服务器分别上传zookeeper安装包，上传到/opt/目录下，然后tar zxvf zookeeper-3.4.10.tar.gz拷贝zoo_sample.cfg 为zoo.cfg 修改/opt/zookeeper-3.4.10/conf/zoo.cfg配置文件，添加如下内容：123server.1=172.16.18.198:2888:3888server.2=172.16.18.199:2888:3888server.3=172.16.18.200:2888:3888修改zookeeper数据文件存放目录1dataDir=/data/zookeeper此时zoo.cfg 配置文件内容为：12345678910111213141516171819202122232425262728293031# The number of milliseconds of each ticktickTime=2000 ##zookeeper单位时间为2ms# The number of ticks that the initial # synchronization phase can takeinitLimit=10 ##对于从节点最初连接到主节点时的超时时间，单位为tick值的倍数。即20ms# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5 ##对于主节点与从节点进行同步操作时的超时时间，单位为tick值的倍数。即10ms# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/data/zookeeper# the port at which the clients will connectclientPort=2181 ##客户端链接端口# the maximum number of client connections.# increase this if you need to handle more clientsmaxClientCnxns=60 ##客户端最大链接数## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1server.1=172.16.18.198:2888:3888 server.2=172.16.18.199:2888:3888server.3=172.16.18.200:2888:3888新建myid文件在三台服务器的数据存放目录下新建myid文件，并写入对应的server.num 中的num数字如：在172.16.18.198上将server.1中1写入myid1echo 1 &gt;/data/zookeeper/myid添加环境变量，方便我们执行脚本命令vi etc/profile 在最后添加如下两个。12export ZOOKEEPER_HOME=/opt/zookeeper-3.4.9export PATH=$PATH:$ZOOKEEPER_HOME/bin:$ZOOKEEPER_HOME/conf保存后重新加载一下：1source /etc/profile修改日志存放目录（可选）vi /opt/zookeeper/bin/zkEnv.sh 找到ZOO_LOG_DIR 和 ZOO_LOG4J_PROP位置1234567891011if [ &quot;x$&#123;ZOO_LOG_DIR&#125;&quot; = &quot;x&quot; ] then #配置zookeeper日志输出存放路径 ZOO_LOG_DIR=&quot;/var/applog/zookeeper&quot; fi if [ &quot;x$&#123;ZOO_LOG4J_PROP&#125;&quot; = &quot;x&quot; ] then #配置日志输出级别,这里把几个级别一并配上 ZOO_LOG4J_PROP=&quot;INFO,CONSOLE,ROLLINGFILE,TRACEFILE&quot; fi编辑conf目录下log4j.properties123456789# Define some default values that can be overridden by system properties zookeeper.root.logger=INFO, CONSOLE, ROLLINGFILE, TRACEFILE zookeeper.console.threshold=INFO zookeeper.log.dir=. zookeeper.log.file=zookeeper.log zookeeper.log.threshold=ERROR zookeeper.tracelog.dir=. zookeeper.tracelog.file=zookeeper_trace.log log4j.rootLogger=$&#123;zookeeper.root.logger&#125;完成log的日志目录的修改。7.启动zookeeper服务zkServer.sh start来启动。zkServer.sh restart (重启)zkServer.sh status (查看状态)zkServer.sh stop (关闭)zkServer.sh start-foreground (以打印日志方式启动)三台服务器分别执行：1zkServer.sh start然后用 status 检查下状态 如果出现 Mode：leader 或者Mode:follower 表示搭建成功。否则前台执行看一下日志。1234$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower如出现：123456789102019-04-29 14:04:05,992 [myid:3] - INFO [ListenerThread:QuorumCnxManager$Listener@739] - My election bind port: /172.16.18.200:38882019-04-29 14:04:06,019 [myid:3] - INFO [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:QuorumPeer@865] - LOOKING2019-04-29 14:04:06,025 [myid:3] - INFO [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@818] - New election. My id = 3, proposed zxid=0x02019-04-29 14:04:06,056 [myid:3] - WARN [WorkerSender[myid=3]:QuorumCnxManager@588] - Cannot open channel to 1 at election address /172.16.18.198:3888java.net.NoRouteToHostException: 没有到主机的路由 at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)&quot;zookeeper.log&quot; 303L, 35429C报这种异常一般有三种情况：1）：zoo.cfg配置文件中，server.x:2888:3888配置出现错误；2）：myid文件内容和server.x不对应，或者myid不在data目录下；3）：系统防火墙是否在启动。我检查了三种原因后发现是防火墙running。centos7下查看防火墙状态的命令：1firewall-cmd --state关闭防火墙的命令：12systemctl stop firewalld.servicesystemctl disable firewalld.service （禁止开机启动，永久关闭防火墙）关闭防火墙后重启即可。验证是否成功在命令行中输入：zkCli.sh -server 172.16.18.198:2181（由于本人在不同的办公地点在修改该文章，所以ip地址也在变化，知道原理即可）即可连接到其中一台ZooKeeper服务器。其他自动实现同步，客户端只需要和一台保持连接即可。出现如下表示链接成功1234WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 172.16.18.198:2181(CONNECTED) 0]","categories":[{"name":"分布式集群","slug":"分布式集群","permalink":"http://www.liuyong520.cn/categories/分布式集群/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.liuyong520.cn/tags/linux/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://www.liuyong520.cn/tags/ZooKeeper/"}]},{"title":"kafka基本介绍","slug":"kafka-intruduce","date":"2019-04-29T02:56:51.000Z","updated":"2019-05-02T11:30:40.781Z","comments":true,"path":"2019/04/29/kafka-intruduce/","link":"","permalink":"http://www.liuyong520.cn/2019/04/29/kafka-intruduce/","excerpt":"","text":"kafka是什么？Kafka是一个分布式流式存储并处理的消息队列。由scale+java语言编写，它提供了类似于JMS的特性，但是在设计实现上又完全不同，因为kafka并不是按照JMS规范实现的。kafka集群由多个broke（Kafka实例称之为broke）组成，在集群里，kafka通过消息订阅和发布将消息以topic的形式发布出来，同时，消息也是存储在topic中的，消息的发送者成为producer，消息接受者成为Consummer。同时，topic 是根据分区partitions，和副本replications来实现的数据的分布式存储，和加强数据的可靠性。何为topic？一个topic可以认为是一类消息，每个topic将被分成多个partitions，每个partition在存储append log的形式存在文件里的。任何发布到partition的消息都会直接被追加到log文件的末尾，每条消息在文件中的位置称之为offset偏移量，offset为一个long型数字，它唯一标识一条消息，kafka并没有提供其他索引来存储offset，因此kafka不支持消息的随机读写。kafka和JMS（Java Message Service）实现(activeMQ)不同的是:即使消息被消费,消息仍然不会被立即删除.日志文件将会根据broker中的配置要求,保留一定的时间之后(默认是7天)删除;比如log文件保留2天,那么两天后,文件会被清除,无论其中的消息是否被消费.kafka通过这种简单的手段,来释放磁盘空间,以及减少消息消费之后对文件内容改动的磁盘IO开支.kafka消息如何消费的？对于consumer而言,它需要保存消费消息的offset,对于offset的保存和使用,有consumer来控制;当consumer正常消费消息时,offset将会”线性”的向前驱动,即消息将依次顺序被消费.事实上consumer可以使用任意顺序消费消息,它只需要将offset重置为任意值..(kafka 老版本中offset将会保存在zookeeper中,1.x之后也会存储在broke集群里,参见下文)kafka 集群里consumer和producer的状态信息是如何保存的？kafka集群几乎不需要维护任何consumer和producer状态信息,这些信息由zookeeper保存;因此producer和consumer的客户端实现非常轻量级,它们可以随意离开,而不会对集群造成额外的影响.kafka为何要引入分区的概念，有何好处？partitions的设计目的有多个.最根本原因是kafka基于文件存储.通过分区,可以将日志内容分散到多个kafka实例上,来避免文件尺寸达到单机磁盘的上限,每个partiton都会被当前server(kafka实例)保存;可以将一个topic切分多任意多个partitions,来消息保存/消费的效率.此外越多的partitions意味着可以容纳更多的consumer,有效提升并发消费的能力.有负载均衡的功效(具体原理参见下文).kakfa数据是如何写入到磁盘的？一个Topic的多个partitions,被分布在kafka集群中的多个server上;每个server(kafka实例)负责partitions中消息的读写操作;此外kafka还可以配置partitions需要备份的个数(replicas),每个partition将会被备份到多台机器上,以提高可用性.基于replicated方案,那么就意味着需要对多个备份进行调度;每个partition都有一个server为”leader”;leader负责所有的读写操作,如果leader失效,那么将会有其他follower来接管(成为新的leader);follower只是单调的和leader跟进,同步消息即可..由此可见作为leader的server承载了全部的请求压力,因此从集群的整体考虑,有多少个partitions就意味着有多少个”leader”,kafka会将”leader”均衡的分散在每个实例上,来确保整体的性能稳定.这和zookeeper的follower是有区别的：zookeeper的follower是可以读到数据的，而kafka的follower是读不到数据的。kafka使用文件存储消息,这就直接决定kafka在性能上严重依赖文件系统的本身特性.且无论任何OS下,对文件系统本身的优化几乎没有可能.文件缓存/直接内存映射等是常用的手段.因为kafka是对日志文件进行append操作,因此磁盘检索的开支是较小的;同时为了减少磁盘写入的次数,broker会将消息暂时buffer起来,当消息的个数(或尺寸)达到一定阀值时,再flush到磁盘,这样减少了磁盘IO调用的次数.kafka中消费者组如何理解？Producer将消息发布到指定的Topic中,同时Producer也能决定将此消息归属于哪个partition;比如基于”round-robin”方式或者通过其他的一些算法等.本质上kafka只支持Topic.每个consumer属于一个consumer group;反过来说,每个group中可以有多个consumer.发送到Topic的消息,只会被订阅此Topic的每个group中的一个consumer消费.如果所有的consumer都具有相同的group,这种情况和queue模式很像;消息将会在consumers之间负载均衡.如果所有的consumer都具有不同的group,那这就是”发布-订阅”;消息将会广播给所有的消费者.在kafka中,一个partition中的消息只会被group中的一个consumer消费;每个group中consumer消息消费互相独立;我们可以认为一个group是一个”订阅”者,一个Topic中的每个partions,只会被一个”订阅者”中的一个consumer消费,不过一个consumer可以消费多个partitions中的消息.kafka只能保证一个partition中的消息被某个consumer消费时,消息是顺序的.事实上,从Topic角度来说,消息仍不是有序的. 因为消费者消费消息的时候是按照分区依次读取的，所以无法保证消息的全局顺序性，只能保证在同一个分区内的消息是顺序的。如果想要所有的消息都是顺序的，可以把分区数设置为1.kafka中如何保证数据一段时间内不丢失？kafka 的producer有ACK机制。可以由用户自行设定是否开启确认机制，如果开启确认机制，kafka会等发送消息到kafka集群时，当leader服务器，会返回元数据给producer客户端，ACK机制也在元数据里，这里的ACK有两种，一种就是leader只要接收成功，就返回确认，另外一种就是：要等所有follower都收到了之后才返回确认。producer在接收到确认之后，才会发下一条消息。而所有的消息最终都是存储在磁盘一段时间的，所以一段时间内消息是不会丢失的。kafka 的应用场景主要有哪些？官方介绍是讲可以用作message queue，数据采集，简单流式计算等。用作消息队列message queue有哪些优缺点？对于一些常规的消息系统,kafka是个不错的选择;partitons/replication和容错,可以使kafka具有良好的扩展性和性能优势.不过到目前为止,我们应该很清楚认识到,kafka并没有提供JMS中的”事务性””消息传输担保(消息确认机制)””消息分组”等企业级特性;kafka只能使用作为”常规”的消息系统,在一定程度上,尚未确保消息的发送与接收绝对可靠(比如,消息重发,消息发送丢失等)kafka是如何保持高性能的？需要考虑的影响性能点很多,除磁盘IO之外,我们还需要考虑网络IO,这直接关系到kafka的吞吐量问题.kafka并没有提供太多高超的技巧;对于producer端,可以将消息buffer起来,当消息的条数达到一定阀值时,批量发送给broker;对于consumer端也是一样,批量fetch多条消息.不过消息量的大小可以通过配置文件来指定.对于kafka broker端,似乎有个sendfile系统调用可以潜在的提升网络IO的性能:将文件的数据映射到系统内存中,socket直接读取相应的内存区域即可,而无需进程再次copy和交换. 其实对于producer/consumer/broker三者而言,CPU的开支应该都不大,因此启用消息压缩机制是一个良好的策略;压缩需要消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑.可以将任何在网络上传输的消息都经过压缩.kafka支持gzip/snappy等多种压缩方式.kafka在消费者端有哪些异常处理策略？对于JMS实现,消息传输担保非常直接:有且只有一次(exactly once).在kafka中稍有不同:1) at most once: 最多一次,这个和JMS中”非持久化”消息类似.发送一次,无论成败,将不会重发.2) at least once: 消息至少发送一次,如果消息未能接受成功,可能会重发,直到接收成功.3) exactly once: 消息只会发送一次.at most once: 消费者fetch消息,然后保存offset,然后处理消息;当client保存offset之后,但是在消息处理过程中出现了异常,导致部分消息未能继续处理.那么此后”未处理”的消息将不能被fetch到,这就是”at most once”.at least once: 消费者fetch消息,然后处理消息,然后保存offset.如果消息处理成功之后,但是在保存offset阶段zookeeper异常导致保存操作未能执行成功,这就导致接下来再次fetch时可能获得上次已经处理过的消息,这就是”at least once”，原因offset没有及时的提交给zookeeper，zookeeper恢复正常还是之前offset状态.exactly once: kafka中并没有严格的去实现基于2阶段提交,事务),我们认为这种策略在kafka中是没有必要的.通常情况下”at-least-once”是我们搜选.(相比at most once而言,重复接收数据总比丢失数据要好).kafka 工作流程是怎样的？主要结构图：大体可以从三个方面分析：生产者产生消息、消费者消费消息、Broker cluster保存消息。生产者产生消息过程分析写入方式：producer 采用push的方式将消息发送到broker cluster，每条消息都被追加到分区中，属于顺序写磁盘（顺序写磁盘效率比随机写内存效率要高，能提高Kafka吞吐率）而且broker集群并不是每一条消息都及时写磁盘，而是先写buffer，达到一定大小或者每隔一段时间再flush到磁盘上。多个producer可以给同一个topic 发布消息，而且可以指定分区发布。分区Partition每个Topic可以有多个分区，而消息最终是存储在磁盘的文件里的，Partition在磁盘上是文件夹的形式存在的。如12345678cd /var/applog/kafka/ ## 赚到kafka数据目录 即log.dir=配置的目录lscleaner-offset-checkpoint __consumer_offsets-22 __consumer_offsets-4 log-start-offset-checkpoint recovery-point-offset-checkpoint__consumer_offsets-1 __consumer_offsets-25 __consumer_offsets-40 meta.properties replication-offset-checkpoint__consumer_offsets-10 __consumer_offsets-28 __consumer_offsets-43 mytest-0 test-0__consumer_offsets-13 __consumer_offsets-31 __consumer_offsets-46 mytest-1__consumer_offsets-16 __consumer_offsets-34 __consumer_offsets-49 mytest-2__consumer_offsets-19 __consumer_offsets-37 __consumer_offsets-7 mytest-3其中mytest-0 mytest-1 mytest-2 mytest-3 即为分区Partition，里面的文件就是分区里面存放的数据。broker cluster 保存消息broker 收到消息后，首先会去找topic对应分区的leader，找到leader后，先将数据写入buffer，再flush到磁盘。然后zookeeper会协调follower自动同步leader分区的数据，以达到replication备份的目的，同时leader会按照备份完成的先后顺序给follower作一次排序，作为leader发生意外时选举时选举为leader的顺序。消费者消费消息消费者消费消息，同一个分区里的数据不能够被一个消费组里面的多个消费者同时消费，同一个消费组里的消费者只能消费不同分区的数据。不同消费者组可以消费同一个分区里的数据。消费者消费数据时是按照分区的一个一个分区数据进行消费的。zookeeper在kafka中的具体作用是什么？kafka是依赖于zookeeper注册中心的，主要来协调各个broker的分区备份，broker的选举，以及消费者相关状信息的存储。kafka使用zookeeper来存储一些meta信息,并使用了zookeeper watch机制来发现meta信息的变更并作出相应的动作(比如consumer失效,触发负载均衡等)1) Broker node registry: 当一个kafkabroker启动后,首先会向zookeeper注册自己的节点信息(临时znode),同时当broker和zookeeper断开连接时,此znode也会被删除.格式: /broker/ids/[0…N] –&gt;host:port;其中[0..N]表示broker id,每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),znode的值为此broker的host:port信息.123456789101112131415161718$ zkCli -server k8s-n1:2181$ ls /brokers[ids, topics, seqid]$ ls /brokers/ids[0, 1, 2]$ get /brokers/ids/0&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLAINTEXT://k8s-n1:9092&quot;],&quot;jmx_port&quot;:-1,&quot;host&quot;:&quot;k8s-n1&quot;,&quot;timestamp&quot;:&quot;1556568752340&quot;,&quot;port&quot;:9092,&quot;version&quot;:4&#125;cZxid = 0xd0000003cctime = Wed Apr 24 16:10:19 CST 2019mZxid = 0xd0000003cmtime = Wed Apr 24 16:10:19 CST 2019pZxid = 0xd0000003ccversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x26a4e173fc40002dataLength = 182numChildren = 02) Broker Topic Registry: 当一个broker启动时,会向zookeeper注册自己持有的topic和partitions信息,仍然是一个临时znode.格式: /broker/topics/[topic]/[0…N] 其中[0..N]表示partition索引号.12$ ls /brokers/topics[test, __consumer_offsets]__consumer_offsets 是消费端的offset12345678910111213141516171819$ ls /brokers/topics/test[partitions] ##test的分区信息$ ls /brokers/topics/test/partitions[0]$ ls /brokers/topics/test/partitions/0[state]$ get /brokers/topics/test/partitions/0/state &#123;&quot;controller_epoch&quot;:19,&quot;leader&quot;:0,&quot;version&quot;:1,&quot;leader_epoch&quot;:3,&quot;isr&quot;:[0]&#125;cZxid = 0x2000000b6ctime = Wed Apr 24 07:53:42 CST 2019mZxid = 0xd00000044mtime = Wed Apr 24 16:10:19 CST 2019pZxid = 0x2000000b6cversion = 0dataVersion = 3aclVersion = 0ephemeralOwner = 0x0dataLength = 73numChildren = 03) Consumer and Consumer group: 每个consumer客户端被创建时,会向zookeeper注册自己的信息;此作用主要是为了”负载均衡”.一个group中的多个consumer可以交错的消费一个topic的所有partitions;简而言之,保证此topic的所有partitions都能被此group所消费,且消费时为了性能考虑,让partition相对均衡的分散到每个consumer上.4) Consumer id Registry: 每个consumer都有一个唯一的ID(host:uuid,可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.格式:/consumers/[group_id]/ids/[consumer_id]仍然是一个临时的znode,此节点的值为{“topic_name”:#streams…},即表示此consumer目前所消费的topic + partitions列表.启动消费者：1$ kafka-console-consumer.sh --bootstrap-server k8s-n2:9092 --topic test启动生成者：1kafka-console-producer.sh --broker-list k8s-n1:9092 --topic test查看zookeeper信息：1234$ ls /[cluster, controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config]$ ls /consumers[]发现consummer下啥也没有？这是因为新版本的kafka，consumer中offset不是放在这个位置的，而是放在__consumer_offset 这个topic下的。那么该如何验证呢？启动消费者：1$ kafka-console-consumer.sh --bootstrap-server k8s-n2:9092 --topic test启动生成者：1kafka-console-producer.sh --broker-list k8s-n1:9092 --topic test验证消息生产成功12345kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list k8s-n1:9092 --topic mytest --time -1mytest:0:15mytest:1:16mytest:2:16mytest:3:15mytest topic 上 0号分区有15条消息。很好理解。再创建一个消费者组1kafka-console-consumer.sh --bootstrap-server k8s-n1:9092 --topic mytest --from-beginning查询一下消费者组信息123kafka-consumer-groups.sh --bootstrap-server k8s-n1:9092 --listconsole-consumer-24766console-consumer-52794查询一下topic里的内容：1kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server k8s-n1:9092 --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --consumer.config config/consumer.properties --from-beginning结果：1234567891011 [console-consumer-52794,__consumer_offsets,12]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,45]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,1]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,5]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,26]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,29]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,34]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,10]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,32]::OffsetAndMetadata(offset=5, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)[console-consumer-52794,__consumer_offsets,40]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1556122524504, expireTimestamp=None)^CProcessed a total of 1674 messages参考了 http://www.cnblogs.com/huxi2b/p/6061110.html这篇blog的作法，但是我的版本是kafka_2.2.0里面并没有找offset的命令。5) Consumer offset Tracking: 用来跟踪每个consumer目前所消费的partition中最大的offset.格式:/consumers/[group_id]/offsets/[topic]/[broker_id-partition_id]–&gt;offset_value此znode为持久节点,可以看出offset跟group_id有关,以表明当group中一个消费者失效,其他consumer可以继续消费.6) Partition Owner registry: 用来标记partition被哪个consumer消费.临时znode格式:/consumers/[group_id]/owners/[topic]/[broker_id-partition_id]–&gt;consumer_node_id当consumer启动时,所触发的操作:A) 首先进行”Consumer id Registry”;B) 然后在”Consumer id Registry”节点下注册一个watch用来监听当前group中其他consumer的”leave”和”join”;只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions).C) 在”Broker id registry”节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance.","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://www.liuyong520.cn/categories/消息队列/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.liuyong520.cn/tags/kafka/"},{"name":"linux","slug":"linux","permalink":"http://www.liuyong520.cn/tags/linux/"}]},{"title":"Linux下kafka集群搭建","slug":"kafka-install","date":"2019-04-29T02:56:51.000Z","updated":"2019-04-30T02:48:31.425Z","comments":true,"path":"2019/04/29/kafka-install/","link":"","permalink":"http://www.liuyong520.cn/2019/04/29/kafka-install/","excerpt":"","text":"环境准备zookeeper集群环境kafka是依赖于zookeeper注册中心的一款分布式消息对列，所以需要有zookeeper单机或者集群环境。三台服务器：123172.16.18.198 k8s-n1172.16.18.199 k8s-n2172.16.18.200 k8s-n3下载kafka安装包http://kafka.apache.org/downloads 中下载，目前最新版本的kafka已经到2.2.0,我这里之前下载的是kafka_2.11-2.2.0.tgz.安装kafka集群上传压缩包到三台服务器解压缩到/opt/目录下12tar -zxvf kafka_2.11-2.2.0.tgz -C /opt/ls -s kafka_2.11-2.2.0 kafka修改 server.properties123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=0############################# Socket Server Settings ############################## The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured.# FORMAT:# listeners = listener_name://host_name:port# EXAMPLE:# listeners = PLAINTEXT://your.host.name:9092listeners=PLAINTEXT://k8s-n1:9092# Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value# returned from java.net.InetAddress.getCanonicalHostName().advertised.listeners=PLAINTEXT://k8s-n1:9092# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL# The number of threads that the server uses for receiving requests from the network and sending responses to the networknum.network.threads=3# The number of threads that the server uses for processing requests, which may include disk I/Onum.io.threads=8# The send buffer (SO_SNDBUF) used by the socket serversocket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket serversocket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)socket.request.max.bytes=104857600############################# Log Basics ############################## A comma separated list of directories under which to store log fileslog.dirs=/var/applog/kafka/# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=5# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.# This value is recommended to be increased for installations with data dirs located in RAID array.num.recovery.threads.per.data.dir=1############################# Internal Topic Settings ############################## The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot;# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync# the OS cache lazily. The following configurations control the flush of data to disk.# There are a few important trade-offs here:# 1. Durability: Unflushed data may be lost if you are not using replication.# 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.# 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.# The settings below allow one to configure the flush policy to flush data after a period of time or# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disklog.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flushlog.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can# be set to delete segments after a period of time, or after a given size has accumulated.# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens# from the end of the log.# The minimum age of a log file to be eligible for deletion due to agelog.retention.hours=24# A size-based retention policy for logs. Segments are pruned from the log unless the remaining# segments drop below log.retention.bytes. Functions independently of log.retention.hours.#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according# to the retention policieslog.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=k8s-n1:2181,k8s-n2:2181,k8s-n3:2181# Timeout in ms for connecting to zookeeperzookeeper.connection.timeout.ms=6000############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.# The default value for this is 3 seconds.# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.group.initial.rebalance.delay.ms=0delete.topic.enable=true拷贝两份到k8s-n2,k8s-n3123456789[root@k8s-n2 config]# cat server.properties broker.id=1listeners=PLAINTEXT://k8s-n2:9092advertised.listeners=PLAINTEXT://k8s-n2:9092[root@k8s-n3 config]# cat server.propertiesbroker.id=2listeners=PLAINTEXT://k8s-n3:9092advertised.listeners=PLAINTEXT://k8s-n3:9092添加环境变量 在/etc/profile 中添加12export ZOOKEEPER_HOME=/opt/kafka_2.11-2.2.0export PATH=$PATH:$ZOOKEEPER_HOME/binsource /etc/profile 重载生效启动kafka1kafka-server-start.sh config/server.properties &amp;Zookeeper+Kafka集群测试创建topic:1kafka-topics.sh --create --zookeeper k8s-n1:2181, k8s-n2:2181, k8s-n3:2181 --replication-factor 3 --partitions 3 --topic test显示topic1kafka-topics.sh --describe --zookeeper k8s-n1:2181, k8s-n2:2181, k8s-n3:2181 --topic test列出topic12kafka-topics.sh --list --zookeeper k8s-n1:2181, k8s-n2:2181, k8s-n3:2181test创建 producer(生产者);12kafka-console-producer.sh --broker-list k8s-n1:9092 --topic testhello创建 consumer（消费者）12kafka-console-consumer.sh --bootstrap-server k8s-n1:9092 --topic test --from-beginninghello至此，kafka集群搭建就已经完成了。","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://www.liuyong520.cn/categories/消息队列/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.liuyong520.cn/tags/kafka/"},{"name":"linux","slug":"linux","permalink":"http://www.liuyong520.cn/tags/linux/"}]},{"title":"hexo博客主题优化","slug":"hexo-promise","date":"2017-08-29T02:56:51.000Z","updated":"2019-04-28T08:58:06.799Z","comments":true,"path":"2017/08/29/hexo-promise/","link":"","permalink":"http://www.liuyong520.cn/2017/08/29/hexo-promise/","excerpt":"","text":"在介绍博客主题优化这个话题之前，我想先介绍hexo主题的大体结构，便于后面将主题优化方面的东西。hexo主题结构我这里选用pure主题为例进行讲解。进入themes/pure文件夹下执行如下命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158$ tree.├── LICENSE├── README.cn.md├── README.md├── _config.yml #主题主配置文件├── _config.yml.example #主题配置文件例子├── _source #博客页面例子文件夹│ ├── 404 #博客404页面只要拷贝到站点soure就行│ │ └── index.md│ ├── _data #博客友情链接页面│ │ ├── gallery.yml│ │ └── links.yml│ ├── about #博客关于页面│ │ └── index.md│ ├── books #博客书单页面│ │ └── index.md│ ├── categories #博客分类页面│ │ └── index.md│ ├── links #博客友情链接│ │ └── index.md│ ├── repository #博客仓库模版页面│ │ └── index.md│ └── tags #博客标签页面│ └── index.md├── languages #博客语言切换配置文件夹│ ├── default.yml│ ├── en.yml│ ├── zh-CN.yml│ └── zh-TW.yml├── layout #博客布局文件夹 这里就是生成页面的精华部分了│ ├── _common│ │ ├── footer.ejs│ │ ├── head.ejs│ │ ├── header.ejs│ │ ├── script.ejs│ │ └── social.ejs│ ├── _partial│ │ ├── archive-book.ejs│ │ ├── archive-category.ejs│ │ ├── archive-link.ejs│ │ ├── archive-list.ejs│ │ ├── archive-post.ejs│ │ ├── archive-repository.ejs│ │ ├── archive-tag.ejs│ │ ├── archive.ejs│ │ ├── article-about.ejs│ │ ├── article.ejs│ │ ├── item-post.ejs│ │ ├── pagination.ejs│ │ ├── post│ │ │ ├── category.ejs│ │ │ ├── comment.ejs│ │ │ ├── copyright.ejs│ │ │ ├── date.ejs│ │ │ ├── donate.ejs│ │ │ ├── gallery.ejs│ │ │ ├── nav.ejs│ │ │ ├── pv.ejs│ │ │ ├── tag.ejs│ │ │ ├── thumbnail.ejs│ │ │ ├── title.ejs│ │ │ └── wordcount.ejs│ │ ├── sidebar-about.ejs│ │ ├── sidebar-toc.ejs│ │ └── sidebar.ejs│ ├── _script│ │ ├── _analytics│ │ │ ├── baidu-analytics.ejs│ │ │ ├── google-analytics.ejs│ │ │ └── tencent-analytics.ejs│ │ ├── _comment│ │ │ ├── disqus.ejs│ │ │ ├── gitalk.ejs│ │ │ ├── gitment.ejs│ │ │ ├── livere.ejs│ │ │ ├── valine.ejs│ │ │ └── youyan.ejs│ │ ├── _search│ │ │ ├── baidu.ejs│ │ │ └── insight.ejs│ │ ├── analytics.ejs│ │ ├── comment.ejs│ │ ├── douban.ejs│ │ ├── fancybox.ejs│ │ ├── mathjax.ejs│ │ ├── pv.ejs│ │ ├── repository.ejs│ │ └── search.ejs│ ├── _search│ │ ├── baidu.ejs│ │ ├── index-mobile.ejs│ │ ├── index.ejs│ │ ├── insight.ejs│ │ └── swiftype.ejs│ ├── _widget│ │ ├── archive.ejs│ │ ├── board.ejs│ │ ├── category.ejs│ │ ├── recent_posts.ejs│ │ ├── tag.ejs│ │ └── tagcloud.ejs│ ├── about.ejs│ ├── archive.ejs│ ├── books.ejs│ ├── categories.ejs│ ├── category.ejs│ ├── index.ejs│ ├── layout.ejs│ ├── links.ejs│ ├── page.ejs│ ├── post.ejs│ ├── repository.ejs│ ├── tag.ejs│ └── tags.ejs├── package.json├── screenshot #主题颜色切换背景│ ├── pure-theme-black.png│ ├── pure-theme-blue.png│ ├── pure-theme-green.png│ ├── pure-theme-purple.png│ ├── pure.png│ └── pure.psd├── scripts│ └── thumbnail.js└── source #主题静态资源文件目录 ├── css │ ├── style.css │ └── style.min.css ├── favicon.png ├── fonts │ ├── README.md │ ├── iconfont.eot │ ├── iconfont.svg │ ├── iconfont.ttf │ └── iconfont.woff ├── images │ ├── avatar.jpg │ ├── avatar.jpg1 │ ├── donate │ │ ├── alipayimg.png │ │ └── wechatpayimg.png │ ├── favatar │ │ ├── SzsFox-logo.png │ │ ├── chuangzaoshi-logo.png │ │ └── idesign-logo.png │ ├── thumb-default.png │ └── xingqiu-qrcode.jpg └── js ├── application.js ├── application.min.js ├── insight.js ├── jquery.min.js ├── plugin.js ├── plugin.js.map └── plugin.min.js29 directories, 125 fileslayout里面的文件使用ejs （js模版语言）ejs官网实现的，里面把整个页面通过js抽取各个小的模块模版文件，同时数据和标签页面是分离的，所以在页面里面可以加载config.yml 里面的配置。整个页面入口文件就是layout.js12345678910111213141516171819202122232425262728&lt;!DOCTYPE html&gt;&lt;html&lt;%= config.language ? &quot; lang=&quot; + config.language.substring(0, 2) : &quot;&quot;%&gt;&gt;&lt;%- partial(&apos;_common/head&apos;, &#123;post: page&#125;) %&gt;##这里会判断是否启用layout配置&lt;% var bodyClass = &apos;main-center&apos;; if (theme.config.layout) &#123; bodyClass = theme.config.layout; &#125; if (theme.config.skin) &#123; bodyClass += &apos; &apos; + theme.config.skin; &#125; bodyClass = page.sidebar === &apos;none&apos; ? (bodyClass + &apos; no-sidebar&apos;) : bodyClass;%&gt;&lt;body class=&quot;&lt;%= bodyClass %&gt;&quot; itemscope itemtype=&quot;http://schema.org/WebPage&quot;&gt; &lt;%- partial(&apos;_common/header&apos;, null, &#123;cache: !config.relative_link&#125;) %&gt; &lt;% if (theme.sidebar &amp;&amp; (page.sidebar!=&apos;none&apos; || page.sidebar!=&apos;custom&apos;))&#123; %&gt; &lt;% if (theme.config.toc &amp;&amp; page.toc)&#123; %&gt; &lt;%- partial(&apos;_partial/sidebar-toc&apos;, &#123;post: page&#125;) %&gt; &lt;% &#125;else&#123; %&gt; &lt;%- partial(&apos;_partial/sidebar&apos;, null, &#123;cache: !config.relative_link&#125;) %&gt; &lt;% &#125; %&gt; &lt;% &#125; %&gt; &lt;%- body %&gt; &lt;%- partial(&apos;_common/footer&apos;, null, &#123;cache: !config.relative_link&#125;) %&gt; &lt;%- partial(&apos;_common/script&apos;, &#123;post: page&#125;) %&gt;&lt;/body&gt;&lt;/html&gt;其中&lt;%- partial(‘_common/footer’, null, {cache: !config.relative_link}) %&gt; 表示引入子模块_common/footer.ejs文件，{cache: !config.relative_link}表示参数我们的创建的博客文章都会加载这个布局文件。我们新创建的博客文章有如下的配置：123456789title: 文章标题categories: - 文章分类tags: - 文章标签toc: true # 是否启用内容索引comment:true #是否启用评论layout:模版文件，如果没有默认不加载任何模版sidebar: none # 是否启用sidebar侧边栏，none：不启用，不配置默认启动以上配置属于page 域的配置文件属于单个页面的，而config.language 这种是全局配置文件（也就是站点配置文件config.yml），每个页面都能使用。theme.config 加载的就是主题的配置文件config.yml 文件。主题配置文件config.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236 # menumenu: Home: . Archives: archives # 归档 Categories: categories # 分类 Tags: tags # 标签 Repository: repository # github repositories Books: books # 豆瓣书单 Links: links # 友链 About: about # 关于# Enable/Disable menu iconsmenu_icons: enable: true # 是否启用导航菜单图标 home: icon-home-fill archives: icon-archives-fill categories: icon-folder tags: icon-tags repository: icon-project books: icon-book-fill links: icon-friendship about: icon-cup-fill# rssrss: /atom.xml# Sitesite: logo: enabled: true width: 40 height: 40 url: ../images/logo.png title: Hexo # 页面title favicon: /favicon.png board: &lt;p&gt;欢迎交流与分享经验!&lt;/p&gt; # 站点公告 copyright: false # 底部版权信息# configconfig: skin: theme-black # 主题颜色 theme-black theme-blue theme-green theme-purple layout: main-center # 布局方式 main-left main-center main-right toc: true # 是否开启文章章节目录导航 menu_highlight: false # 是否开启当前菜单高亮显示 thumbnail: false # enable posts thumbnail, options: true, false excerpt_link: Read More# Pagination 分页pagination: number: false #是否开启数字 prev: alwayShow: true next: alwayShow: true# Sidebarsidebar: rightwidgets: - board - category - tag - tagcloud - archive - recent_posts# display widgets at the bottom of index pages (pagination == 2)index_widgets:# - category# - tagcloud# - archive # widget behaviorarchive_type: &apos;monthly&apos;show_count: true# Fancyboxfancybox: false# Searchsearch: insight: true # you need to install `hexo-generator-json-content` before using Insight Search baidu: false # you need to disable other search engines to use Baidu search, options: true, false# Donatedonate: enable: true # 微信打赏 wechatpay: qrcode: images/donate/wechatpayimg.png title: 微信支付 # 支付宝打赏 alipay: qrcode: images/donate/alipayimg.png title: 支付宝# Share# weibo,qq,qzone,wechat,tencent,douban,diandian,facebook,twitter,google,linkedinshare: enable: true # 是否启用分享 sites: weibo,qq,wechat,facebook,twitter # PC端显示的分享图标 mobile_sites: weibo,qq,qzone # 移动端显示的分享图标# Githubgithub: username: ***# Comment# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/comment: type: youyan disqus: # enter disqus shortname here youyan: uid: 1783844 # enter youyan uid livere: uid: # enter youyan uid gitment: githubID: repo: ClientID: ClientSecret: lazy: false gitalk: # gitalk. https://gitalk.github.io/ owner: #必须. GitHub repository 所有者，可以是个人或者组织。 admin: #必须. GitHub repository 的所有者和合作者 (对这个 repository 有写权限的用户)。 repo: #必须. GitHub repository. ClientID: #必须. GitHub Application Client ID. ClientSecret: #必须. GitHub Application Client Secret. valine: # Valine. https://valine.js.org appid: # your leancloud application appid appkey: # your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style meta: nick,mail,link # custom comment header pageSize: 10 # pagination size visitor: false # Article reading statistic https://valine.js.org/visitor.html# douban 豆瓣书单# Api： # https://developers.douban.com/wiki/?title=book_v2 图书 # https://developers.douban.com/wiki/?title=movie_v2 电影# books： # https://api.douban.com/v2/book/user/:name/collections?start=0&amp;count=100 个人书单列表# movies: # https://api.douban.com/v2/movie/in_theaters 正在上映的电影 # https://api.douban.com/v2/movie/coming_soon 即将上映的电影 # https://api.douban.com/v2/movie/subject/:id 单个电影信息 # https://api.douban.com/v2/movie/search?q=&#123;text&#125; 电影搜索douban: user: # 豆瓣用户名 start: 0 # 从哪一条记录开始 count: 100 # 获取豆瓣书单数据条数 # PVpv: busuanzi: enable: false # 不蒜子统计 leancloud: enable: false # leancloud统计 app_id: # leancloud &lt;AppID&gt; app_key: # leancloud &lt;AppKey&gt; # wordcountpostCount: enable: false wordcount: true # 文章字数统计 min2read: true # 阅读时长预计 # Pluginsplugins: google_analytics: # enter the tracking ID for your Google Analytics google_site_verification: # enter Google site verification code baidu_analytics: # enter Baidu Analytics hash key tencent_analytics: # Miscellaneoustwitter:google_plus:fb_admins:fb_app_id: # profileprofile: enabled: true # Whether to show profile bar avatar: images/avatar.jpg gravatar: # Gravatar email address, if you enable Gravatar, your avatar config will be overriden author: 昵称 author_title: Web Developer &amp; Designer author_description: 个人简介。 location: Shenzhen, China follow: https://github.com/cofess # Social Links social: links: github: https://github.com/cofess weibo: http://weibo.com/cofess twitter: https://twitter.com/iwebued # facebook: / # dribbble: / behance: https://www.behance.net/cofess rss: atom.xml link_tooltip: true # enable the social link tooltip, options: true, false # My Skills skills: Git: ★★★☆☆ Gulp: ★★★☆☆ Javascript: ★★★☆☆ HTML+CSS: ★★★☆☆ Bootstrap: ★★★☆☆ ThinkPHP: ★★★☆☆ 平面设计: ★★★☆☆ # My Personal Links links: Github: https://github.com/cofess Blog: http://blog.cofess.com 微博: http://weibo.com/cofess 花瓣: http://huaban.com/cofess Behance: https://www.behance.net/cofess # My Personal Labels labels: - 前端 - 前端开发 - 前端重构 - Web前端 - 网页重构 # My Personal Works works: name: link: http://www.example.com date: 2016 # My Personal Projects projects: cofess/gulp-startpro: https://github.com/cofess/gulp-startpro cofess/hexo-theme-pure: https://github.com/cofess/hexo-theme-pure基本上每个配置做什么用的，配置文件里面基本写了注解。也很容易理解。如果还不是很能理解配置项。可以查看https://github.com/cofess/hexo-theme-pure/blob/master/README.cn.md 文件。至此，hexo模版的大体结构已经清楚了。主题优化修改主题在config.yml 文件中修改1234 # Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: pure修改语言在config.yml 文件中修改12# Sitelanguage: zh-CN #修改成中文添加Rss订阅安装feed插件1npm install hexo-generator-feed --save在config.yml添加12345 # Extensions## Plugins: https://hexo.io/plugins/#RSS订阅plugin:- hexo-generator-feed设置feed插件参数12345 #Feed Atomfeed: type: atom path: atom.xml limit: 20生成预览12hexo ghexo d预览下就是如下添加站点地图站点地图是一种文件，您可以通过该文件列出您网站上的网页，从而将您网站内容的组织架构告知Google和其他搜索引擎。Googlebot等搜索引擎网页抓取工具会读取此文件，以便更加智能地抓取您的网站分别安装百度和google插件12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save在博客目录的_config.yml中添加如下代码12345# 自动生成sitemapsitemap:path: sitemap.xmlbaidusitemap:path: baidusitemap.xml编译你的博客12hexo ghexo s如果你在你的博客根目录的public下面发现生成了sitemap.xml以及baidusitemap.xml就表示成功了,在本地访问 http://127.0.0.4000/sitemap.xml 和 http://127.0.0.4000/baidusitemap.xml 就能正确的展示出两个sitemap 文件了。注册百度站长平台4.1 访问：https://ziyuan.baidu.com/linksubmit/index4.2 提交链接提交链接方式有主动推送、自动推送、sitemap、手动上传等。4.3主动推送安装对应提交插件1npm install hexo-baidu-url-submit --save修改配置：123456789101112131415##配置插件plugin:- hexo-generator-baidu-sitemap- hexo-generator-sitemap- hexo-baidu-url-submitbaidu_url_submit: ## 比如3，代表提交最新的三个链接 count: 3 # 在百度站长平台中注册的域名 host: www.liuyong520.cn ## 请注意这是您的秘钥， 请不要发布在公众仓库里! token: upR0BjzCYxTC2CPq ## 文本文档的地址， 新链接会保存在此文本文档里 path: baidu_urls.txt编译博客12hexo ghexo d如果出现下图即表示成功了4.4 自动推送将如下代码添加到head.ejs中即可生效1234567891011121314&lt;script&gt; (function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s); &#125;)(); &lt;/script&gt;4.5 sitemap 提交方式打开百度站长平台，点击sitemap，填入我们的sitemap文件路径：&lt;域名&gt;/&lt;sitemap名字&gt;如下提交即可.但是此时你的域名其实并没有被百度站长所收录：百度依然检索不到你的网站，需要10多个工作日之后才能审核通过。绑定站点到熊掌ID，这样熊掌ID站点管理里面就能看到相关站点数据了登录站长平台，注册熊掌ID，提交审核过后点击站点收录：静态资源压缩hexo 的文章是通过md格式的文件经过swig转换成的html，生成的html会有很多空格，而且自己写的js以及css中会有很多的空格和注释。js和java不一样，注释也会影响一部分的性能，空格同样是的。静态资源压缩也有多种手段：有gulp插件和hexo自带的neat插件。1.hexo-neat 插件：安装hexo-neat插件1npm install hexo-neat --save修改站点配置文件_config.yml：12345678910111213141516171819202122 # hexo-neat# 博文压缩neat_enable: true# 压缩htmlneat_html: enable: true exclude:# 压缩css neat_css: enable: true exclude: - &apos;**/*.min.css&apos;# 压缩jsneat_js: enable: true mangle: true output: compress: exclude: - &apos;**/*.min.js&apos; - &apos;**/jquery.fancybox.pack.js&apos; - &apos;**/index.js&apos;编译博客12hexo g hexo dgulp插件方式安装gulp及相关插件123456npm install gulp -gnpm install gulp-minify-css --savenpm install gulp-uglify --savenpm install gulp-htmlmin --savenpm install gulp-htmlclean --savenpm install gulp-imagemin --save在 Hexo 站点下新建 gulpfile.js文件，文件内容如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445var gulp = require(&apos;gulp&apos;);var minifycss = require(&apos;gulp-minify-css&apos;);var uglify = require(&apos;gulp-uglify&apos;);var htmlmin = require(&apos;gulp-htmlmin&apos;);var htmlclean = require(&apos;gulp-htmlclean&apos;);var imagemin = require(&apos;gulp-imagemin&apos;);// 压缩css文件gulp.task(&apos;minify-css&apos;, function() &#123; return gulp.src(&apos;./public/**/*.css&apos;) .pipe(minifycss()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 压缩html文件gulp.task(&apos;minify-html&apos;, function() &#123; return gulp.src(&apos;./public/**/*.html&apos;) .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest(&apos;./public&apos;))&#125;);// 压缩js文件gulp.task(&apos;minify-js&apos;, function() &#123; return gulp.src([&apos;./public/**/.js&apos;,&apos;!./public/js/**/*min.js&apos;]) .pipe(uglify()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 压缩 public/demo 目录内图片gulp.task(&apos;minify-images&apos;, function() &#123; gulp.src(&apos;./public/demo/**/*.*&apos;) .pipe(imagemin(&#123; optimizationLevel: 5, //类型：Number 默认：3 取值范围：0-7（优化等级） progressive: true, //类型：Boolean 默认：false 无损压缩jpg图片 interlaced: false, //类型：Boolean 默认：false 隔行扫描gif进行渲染 multipass: false, //类型：Boolean 默认：false 多次优化svg直到完全优化 &#125;)) .pipe(gulp.dest(&apos;./public/uploads&apos;));&#125;);// 默认任务gulp.task(&apos;default&apos;, [ &apos;minify-html&apos;,&apos;minify-css&apos;,&apos;minify-js&apos;,&apos;minify-images&apos;]);只需要每次在执行 generate 命令后执行 gulp 就可以实现对静态资源的压缩，压缩完成后执行 deploy 命令同步到服务器：123hexo ggulphexo d修改访问URL路径默认情况下访问URL路径为：domain/2018/10/18/关于本站,修改为 domain/About/关于本站。 编辑 Hexo 站点下的 _config.yml 文件，修改其中的 permalink字段：1permalink: :category/:title/博文置顶安装插件12npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save然后在需要置顶的文章的Front-matter中加上top即可：12345--title: 2018date: 2018-10-25 16:10:03top: 10---设置置顶标志打开：/themes/*/layout/_macro/post.swig，定位到12345&#123;% if post.top %&#125; &lt;i class=&quot;fa fa-thumb-tack&quot;&gt;&lt;/i&gt; &lt;font color=7D26CD&gt;置顶&lt;/font&gt; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;&#123;% endif %&#125;","categories":[{"name":"hexo","slug":"hexo","permalink":"http://www.liuyong520.cn/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.liuyong520.cn/tags/hexo/"}]},{"title":"利用hexo搭建博客","slug":"creatblog","date":"2017-08-27T02:56:51.000Z","updated":"2019-04-27T16:22:09.846Z","comments":true,"path":"2017/08/27/creatblog/","link":"","permalink":"http://www.liuyong520.cn/2017/08/27/creatblog/","excerpt":"","text":"如果你和我一样是小白，那么恭喜你！看完这篇文章，你也可以拥有一个这样的博客前面已经介绍过如何搭建hexo环境，现在我将介绍如何用hexo搭建自己的blog博客搭建实施方案方案一：GithubPages创建Github账号创建仓库 ，仓库名为：&lt;Github账号名称&gt;.github.io点击settings往下翻就能看到githubPages，我这里是已经配置过了的，没有配置可以是select themes ，点击能够选择SkyII主题。（SkyII主题也是和hexo类似的blog的框架，这里不与介绍）将本地Hexo博客推送到GithubPages3.1. 安装hexo-deployer-git插件。在命令行（即Git Bash）运行以下命令即可：1$ npm install hexo-deployer-git --save3.2. 添加SSH key。创建一个 SSH key 。在命令行（即Git Bash）输入以下命令， 回车三下即可：1$ ssh-keygen -t rsa -C &quot;邮箱地址&quot;添加到 github。 复制密钥文件内容（路径形如C:\\Users\\Administrator.ssh\\id_rsa.pub），粘贴到New SSH Key即可。测试是否添加成功。在命令行（即Git Bash）依次输入以下命令，返回“You’ve successfully authenticated”即成功：1ssh -T git@github.com3.3. 修改_config.yml（在站点目录下）。文件末尾修改为：123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:&lt;Github账号名称&gt;/&lt;Github账号名称&gt;.github.io.git branch: master注意：上面仓库地址写ssh地址，不写http地址。3.4. 推送到GithubPages。在命令行（即Git Bash）依次输入以下命令， 返回INFO Deploy done: git即成功推送：12$ hexo g$ hexo d等待1分钟左右，浏览器访问网址： https://&lt;Github账号名称&gt;.github.io至此，您的Hexo博客已经搭建在GithubPages, 域名为https://&lt;Github账号名称&gt;.github.io。方案二：GithubPages + 域名在方案一的基础上，添加自定义域名（您购买的域名）。我的是从阿里云购买的。域名解析类型选择为 CNAME；主机记录即域名前缀，填写为www；记录值填写为&lt;Github账号名称&gt;.github.io；解析线路，TTL 默认即可点击 liuyong520.cn仓库设置。2.1. 打开博客仓库设置：https://github.com/&lt;Github账号名称&gt;/&lt;Github账号名称&gt;.github.io/settings2.2. 在Custom domain下，填写自定义域名，点击save。2.3. 在站点目录的source文件夹下，创建并打开CNAME.txt，写入你的域名（如www.liuyong520.cn），保存，并重命名为CNAME。如图等待10分钟左右。浏览器访问自定义域名。http://www.liuyong520.cn至此，您的Hexo博客已经解析到自定义域名，https://&lt;Github账号名称&gt;.github.io依然可用。方案三：GithubPages + CodingPages + 域名GithubPages 在国内较慢，百度不收录，而CodingPages 在国外较快。所以在方案二的基础上，添加CodingPages 。创建Coding账号创建仓库， 仓库名为：&lt;Coding账号名称&gt;进入项目里『代码』页面，点击『一键开启静态 Pages』，稍等片刻CodingPages即可部署成功。将本地Hexo博客推送到CodingPages4.1. 鉴于创建GithubPages 时，已经生成过公钥。可直接复制密钥文件内容（路径形如C:\\Users\\Administrator.ssh\\id_rsa.pub）， 粘贴到新增公钥。4.2. 测试是否添加成功。在命令行（即Git Bash）依次输入以下命令，返回“You’ve successfully authenticated”即成功：12$ ssh -T git@git.coding.net$ yes4.3. 修改_config.yml（在存放Hexo初始化文件的路径下）。文件末尾修改为：123456789# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:- type: git repo: git@github.com:&lt;Github账号名称&gt;/&lt;Github账号名称&gt;.github.io.git branch: master- type: git repo: git@git.dev.tencent.com:&lt;Coding账号名称&gt;/&lt;Coding账号名称&gt;.git branch: master4.4. 推送到GithubPages。在命令行（即Git Bash）依次输入以下命令， 返回INFO Deploy done: git即成功推送：12$ hexo g$ hexo d域名解析添加 CNAME 记录指向 &lt;Coding账号名称&gt;.coding.me类型选择为 CNAME；主机记录即域名前缀，填写为www；记录值填写为&lt;Github账号名称&gt;.coding.me；解析线路，TTL 默认即可。添加 两条A 记录指向 192.30.252.153和192.30.252.154类型选择为 A；主机记录即域名前缀，填写为@；记录值填写为192.30.252.153和192.30.252.154；解析线路，境外或谷歌。在『Pages 服务』设置页（https://dev.tencent.com/u/&lt;Coding账号名称&gt;/p/&lt;Coding账号名称&gt;/git/pages/settings）中绑定自定义域名至此，您的Hexo博客已经解析到自定义域名，https://&lt;Github账号名称&gt;.github.io和https://&lt;Coding账号名称&gt;.coding.me依然可用。切换主题选择主题hexo主题是非常多的，默认的主题是landscape，您可以自主的在hexo官方网站上挑选自己喜欢的主题，网站：https://hexo.io/themes/推荐以下主题：snippetHieroJsimpleBlueLakePureNextHueman我这里选择的是Pure。1git clone https://github.com/cofess/hexo-theme-pure.git themes/pure此时会在themes 目录下生成 pure目录应用主题更改站点配置_config.yml 修改成1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: &lt;主题文件夹的名称&gt;主题优化以上主题都有比较详细的说明文档，本节主要解决主题优化的常见问题。主题优化一般包括：设置「RSS」添加「标签」页面添加「分类」页面设置「字体」设置「代码高亮主题」侧边栏社交链接开启打赏功能设置友情链接腾讯公益404页面站点建立时间订阅微信公众号设置「动画效果」设置「背景动画」下一次我将针对Pure进行主题方面的相关配置，以及讲解一下hexo主题的的实现原理的。这样你们针对不同的主题也就都能配置了。","categories":[{"name":"hexo","slug":"hexo","permalink":"http://www.liuyong520.cn/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.liuyong520.cn/tags/hexo/"}]},{"title":"Hexo之环境搭建","slug":"hexo-install","date":"2017-08-27T02:56:51.000Z","updated":"2019-04-27T12:11:10.273Z","comments":true,"path":"2017/08/27/hexo-install/","link":"","permalink":"http://www.liuyong520.cn/2017/08/27/hexo-install/","excerpt":"","text":"如果你和我一样是小白，那么恭喜你！看完这篇文章，你也可以拥有一个这样的博客啦！前言在以前我们要维护一个专属于自己的blog，是比较麻烦的，要购买服务器，部署博客程序到服务器，还要维护相关数据和网络。这一类blog最为典型的例子就是WordPress。而今天我们要介绍的是如何基于Hexo博客快速的搭建我们自己服务器系列。hexo介绍Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。hexo安装hexo 是基于node.js环境的，所以安装前，您必须检查电脑中是否已安装下列应用程序：node.js如果您的电脑中已经安装上述必备程序，那么恭喜您！接下来只需要使用 npm 即可完成 Hexo 的安装。1$ npm install -g hexo-cli如果您的电脑中未安装Node，那么就需要安装Node.js详细安装步骤参考：http://www.liuyong520.cn/2017/08/26/nodejs-install/再安装Hexo，在命令行（即Git Bash）运行以下命令：1npm install -g hexo-cli至此Hexo的环境就搭建好了，下一步验证一下hexo123456789101112131415161718MacBook-Pro:_posts xxydliuyss$ hexo versionhexo: 3.8.0hexo-cli: 1.1.0os: Darwin 18.5.0 darwin x64http_parser: 2.8.0node: 10.15.3v8: 6.8.275.32-node.51uv: 1.23.2zlib: 1.2.11ares: 1.15.0modules: 64nghttp2: 1.34.0napi: 3openssl: 1.1.0jicu: 62.1unicode: 11.0cldr: 33.1tz: 2018e这样hexo就安装完成了hexo命令介绍官网已经介绍的比较详细了这里就不再赘述了详情请看官方命令地址：https://hexo.io/zh-cn/docs/commandshexo快速新建博客初始化Hexo，在命令行（即Git Bash）依次运行以下命令即可：以下，即存放Hexo初始化文件的路径， 即站点目录。123$ hexo init myproject$ cd myproject$ npm install新建完成后，在路径下，会产生这些文件和文件夹：123456789$ tree.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes目录名或者文件名详情介绍_config.ymlhexo 全局配置文件package.jsonnodejs 包配置文件scaffoldshexo模版文件夹hexo new filename 会对应根据模版文件生成文件source项目源代码文件目录_drafts为草稿原文件目录_posts项目发布文件目录 项目最终会根据这个目录下的文件生成htmlthemes博客主题存放目录注：hexo相关命令均在站点目录下，用Git Bash运行。站点配置文件：站点目录下的_config.yml。​ 路径为_config.yml主题配置文件：站点目录下的themes文件夹下的，主题文件夹下的_config.yml。​ 路径为\\themes\\&lt;主题文件夹&gt;_config.yml启动服务器。在路径下，命令行（即Git Bash）输入以下命令，运行即可：1hexo server浏览器访问网址： http://localhost:4000/ 就可以预览博客了下一篇 我将介绍如何搭建自己的blog","categories":[{"name":"hexo","slug":"hexo","permalink":"http://www.liuyong520.cn/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.liuyong520.cn/tags/hexo/"}]},{"title":"node.js环境搭建","slug":"nodejs-install","date":"2017-08-26T01:56:51.000Z","updated":"2019-04-27T09:42:47.606Z","comments":true,"path":"2017/08/26/nodejs-install/","link":"","permalink":"http://www.liuyong520.cn/2017/08/26/nodejs-install/","excerpt":"","text":"安装node.js登录官网下载对应的exe安装包。下载地址为：你可以根据不同平台系统选择你需要的Node.js安装包。Node.js 历史版本下载地址：https://nodejs.org/dist/注意：Linux上安装Node.js需要安装Python 2.6 或 2.7 ，不建议安装Python 3.0以上版本。windows 上安装 node.js你可以采用以下两种方式来安装。1、Windows 安装包(.msi)32 位安装包下载地址 : https://nodejs.org/dist/v4.4.3/node-v4.4.3-x86.msi64 位安装包下载地址 : https://nodejs.org/dist/v4.4.3/node-v4.4.3-x64.msi本文实例以 v0.10.26 版本为例，其他版本类似， 安装步骤：步骤 1 : 双击下载后的安装包 v0.10.26，如下所示：步骤 2: 点击以上的Run(运行)，将出现如下界面：步骤 3 : 勾选接受协议选项，点击 next（下一步） 按钮 :步骤 4 : Node.js默认安装目录为 “C:\\Program Files\\nodejs\\” , 你可以修改目录，并点击 next（下一步）：步骤 5 : 点击树形图标来选择你需要的安装模式 , 然后点击下一步 next（下一步）步骤 6 :点击 Install（安装） 开始安装Node.js。你也可以点击 Back（返回）来修改先前的配置。 然后并点击 next（下一步）：点击 Finish（完成）按钮退出安装向导。检测PATH环境变量是否配置了Node.js，点击开始=》运行=》输入”cmd” =&gt; 输入命令”path”，输出如下结果：12345PATH=C:\\oraclexe\\app\\oracle\\product\\10.2.0\\server\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;c:\\python32\\python;C:\\MinGW\\bin;C:\\Program Files\\GTK2-Runtime\\lib;C:\\Program Files\\MySQL\\MySQL Server 5.5\\bin;C:\\Program Files\\nodejs\\;C:\\Users\\rg\\AppData\\Roaming\\npm我们可以看到环境变量中已经包含了C:\\Program Files\\nodejs\\检查Node.js版本12E:\\&gt; node --versionv0.10.262、Windows 二进制文件 (.exe)安装32 位安装包下载地址 : http://nodejs.org/dist/v0.10.26/node.exe64 位安装包下载地址 : http://nodejs.org/dist/v0.10.26/x64/node.exe安装步骤步骤 1 : 双击下载的安装包 Node.exe ，将出现如下界面 :点击 Run（运行）按钮将出现命令行窗口：版本测试进入 node.exe 所在的目录，如下所示如果你获得以上输出结果，说明你已经成功安装了Node.js。linux安装node.js直接使用已编译好的包Node 官网已经把 linux 下载版本更改为已编译好的版本了，我们可以直接下载解压后使用：12345# wget https://nodejs.org/dist/v10.9.0/node-v10.9.0-linux-x64.tar.xz // 下载# tar xf node-v10.9.0-linux-x64.tar.xz // 解压# cd node-v10.9.0-linux-x64/ // 进入解压目录# ./bin/node -v // 执行node命令 查看版本v10.9.0解压文件的 bin 目录底下包含了 node、npm 等命令，我们可以使用 ln 命令来设置软连接12ln -s /usr/software/nodejs/bin/npm /usr/local/bin/ ln -s /usr/software/nodejs/bin/node /usr/local/bin/Ubuntu 源码安装 Node.js以下部分我们将介绍在 Ubuntu Linux 下使用源码安装 Node.js 。 其他的 Linux 系统，如 Centos 等类似如下安装步骤。在 Github 上获取 Node.js 源码：12$ sudo git clone https://github.com/nodejs/node.gitCloning into &apos;node&apos;...修改目录权限：1$ sudo chmod -R 755 node使用 ./configure 创建编译文件，并按照：1234$ cd node$ sudo ./configure$ sudo make$ sudo make install查看 node 版本：12$ node --versionv0.10.25Ubuntu apt-get命令安装命令格式如下：12sudo apt-get install nodejssudo apt-get install npmCentOS 下源码安装 Node.js1、下载源码，你需要在https://nodejs.org/en/download/ 下载最新的Nodejs版本，本文以v0.10.24为例:12cd /usr/local/src/wget http://nodejs.org/dist/v0.10.24/node-v0.10.24.tar.gz2、解压源码1tar zxvf node-v0.10.24.tar.gz3、 编译安装1234cd node-v0.10.24./configure --prefix=/usr/local/node/0.10.24makemake install4、 配置NODE_HOME，进入profile编辑环境变量1vim /etc/profile设置nodejs环境变量，在 export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL 一行的上面添加如下内容:123#set for nodejsexport NODE_HOME=/usr/local/node/0.10.24export PATH=$NODE_HOME/bin:$PATH:wq保存并退出，编译/etc/profile 使配置生效1source /etc/profile验证是否安装配置成功1node -v输出 v0.10.24 表示配置成功npm模块安装路径:1/usr/local/node/0.10.24/lib/node_modules/注：Nodejs 官网提供了编译好的Linux二进制包，你也可以下载下来直接应用。Mac OS 上安装你可以通过以下两种方式在 Mac OS 上来安装 node：1、在官方下载网站下载 pkg 安装包，直接点击安装即可。2、使用 brew 命令来安装：1brew install node","categories":[{"name":"node","slug":"node","permalink":"http://www.liuyong520.cn/categories/node/"}],"tags":[{"name":"node","slug":"node","permalink":"http://www.liuyong520.cn/tags/node/"}]},{"title":"xsync 同步命令脚本和xcall远程执行命令脚本","slug":"xsync","date":"2017-03-28T16:18:32.000Z","updated":"2019-04-28T16:40:15.580Z","comments":true,"path":"2017/03/29/xsync/","link":"","permalink":"http://www.liuyong520.cn/2017/03/29/xsync/","excerpt":"","text":"缘由在linux服务器集群上，有时我们需要将数据从主服务器同步到所有的从服务器上，或者在集群里需要执行一条或者多条命令，如果们一次次的拷贝，或者每个服务器一条条的执行，会造成重复的工作。所以就写两个脚本解决这方面的问题。xsync命令的编写安装 sync命令1yum install -y sync编写脚本 environment.sh12345#! /usr/bin/bash# 集群 IP 数组export NODE_IPS=(172.16.18.198 172.16.18.199 172.16.18.200)# 集群各 IP 对应的 主机名数组export NODE_NAMES=(k8s-n1 k8s-n2 k8s-n3)编写xsyncj考本1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/bin/bash# 获取输出参数，如果没有参数则直接返回pcount=$#if [ $pcount -eq 0 ]then echo &quot;no parameter find !&quot;; exit;fi# 获取传输文件名p1=$1filename=`basename $p1`echo &quot;load file $p1 success !&quot;# 获取文件的绝对路径pdir=`cd -P $(dirname $p1); pwd`echo &quot;file path is $pdir&quot;# 获取当前用户（如果想使用root用户权限拷贝文件，在命令后加入-root参数即可）user=$2case &quot;$user&quot; in&quot;-root&quot;) user=&quot;root&quot;;;&quot;&quot;) user=`whoami`;;*) echo &quot;illegal parameter $user&quot; esacecho $user# 拷贝文件到从机(这里注意主机的host需要根据你的实际情况配置，要与你具体的主机名对应)source /opt/user/environment.shindex=0for node_ip in $&#123;NODE_IPS[@]&#125;do echo &quot;================current host is $&#123;NODE_NAMES[$index]&#125; ip is $&#123;node_ip&#125;=================&quot; rsync -rvl $pdir/$filename $user@$&#123;node_ip&#125;:$pdir index=`expr $index + 1`doneecho &quot;complete !&quot;xcall脚本的编写利用ssh命令远程执行脚本命令。脚本如下：12345678910111213141516171819202122232425#!/bin/bash# 获取控制台指令cmd=$*# 判断指令是否为空if (( #$cmd -eq # ))then echo &quot;command can not be null !&quot; exitfi# 获取当前登录用户user=`whoami`source /opt/user/environment.sh# 在从机执行指令,这里需要根据你具体的集群情况配置，host与具体主机名一致for node_ip in $&#123;NODE_IPS[@]&#125;do echo &quot;================current host is $&#123;node_ip&#125;=================&quot; echo &quot;--&gt; excute command \\&quot;$cmd\\&quot;&quot; ssh $user@$&#123;node_ip&#125; $cmddoneecho &quot;excute successfully !&quot;这两个脚本仅仅只是一个简单的脚本，欢迎大家修改和使用。","categories":[{"name":"linux shell","slug":"linux-shell","permalink":"http://www.liuyong520.cn/categories/linux-shell/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.liuyong520.cn/tags/linux/"},{"name":"shell","slug":"shell","permalink":"http://www.liuyong520.cn/tags/shell/"}]}]}